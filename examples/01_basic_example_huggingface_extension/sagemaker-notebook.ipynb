{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Getting Started Demo\n",
    "### Binary Classification with `Trainer` and `imdb` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introduction](#Introduction)  \n",
    "[Development Environment and Permissions](#Development-Environment-and-Permissions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to our end-to-end binary Text-Classification example. In this demo, we will use the Hugging Faces `transformers` and `datasets` library togehter with a custom Amazon sagemaker-sdk extension to fine-tune a pre-trained transformer on binary text classification. In particular, the pre-trained model will be fine-tuned using `imdb` dataset. To get started, we need to set up the environment with a few prerequisite steps, for permissions, configurations, and so on. \n",
    "\n",
    "_**NOTE: You can run this demo in Sagemaker Studio or on you local machine**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "runtime='local'\n",
    "\n",
    "if 'SAGEMAKER_TRAINING_MODULE' in os.environ:\n",
    "    runtime='sagemaker'\n",
    "    \n",
    "runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you run this demo within sagemaker studio, you have to update the `ipywidgets` for the `datasets` library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os \n",
    "import IPython\n",
    "if runtime =='sagemaker':\n",
    "    !conda install -c conda-forge ipywidgets -y\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/philschmid/sagemaker-sdk-huggingface.git\n",
      "  Cloning https://github.com/philschmid/sagemaker-sdk-huggingface.git to /private/var/folders/jj/dzns9hc55db1vmfsjvrh9n8m0000gp/T/pip-req-build-0mgcmq3f\n",
      "Requirement already satisfied: sagemaker in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker-huggingface==0.0.1) (2.23.0)\n",
      "Requirement already satisfied: sagemaker-experiments in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker-huggingface==0.0.1) (0.1.25)\n",
      "Requirement already satisfied: torch==1.6.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker-huggingface==0.0.1) (1.6.0)\n",
      "Requirement already satisfied: transformers in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker-huggingface==0.0.1) (4.1.1)\n",
      "Requirement already satisfied: datasets in /Users/philippschmid/projects/personal/huggingface/datasets/src (from sagemaker-huggingface==0.0.1) (1.2.1)\n",
      "Requirement already satisfied: sklearn in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker-huggingface==0.0.1) (0.0)\n",
      "Requirement already satisfied: boto3 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker-huggingface==0.0.1) (1.16.43)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker-huggingface==0.0.1) (1.19.4)\n",
      "Requirement already satisfied: matplotlib in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker-huggingface==0.0.1) (3.3.3)\n",
      "Requirement already satisfied: future in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from torch==1.6.0->sagemaker-huggingface==0.0.1) (0.18.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from boto3->sagemaker-huggingface==0.0.1) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.43 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from boto3->sagemaker-huggingface==0.0.1) (1.19.43)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from boto3->sagemaker-huggingface==0.0.1) (0.3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from botocore<1.20.0,>=1.19.43->boto3->sagemaker-huggingface==0.0.1) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from botocore<1.20.0,>=1.19.43->boto3->sagemaker-huggingface==0.0.1) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.43->boto3->sagemaker-huggingface==0.0.1) (1.15.0)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from datasets->sagemaker-huggingface==0.0.1) (0.17.1)\n",
      "Requirement already satisfied: dill in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from datasets->sagemaker-huggingface==0.0.1) (0.3.1.1)\n",
      "Requirement already satisfied: pandas in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from datasets->sagemaker-huggingface==0.0.1) (1.1.5)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from datasets->sagemaker-huggingface==0.0.1) (2.25.1)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from datasets->sagemaker-huggingface==0.0.1) (4.49.0)\n",
      "Requirement already satisfied: xxhash in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from datasets->sagemaker-huggingface==0.0.1) (2.0.0)\n",
      "Requirement already satisfied: multiprocess in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from datasets->sagemaker-huggingface==0.0.1) (0.70.9)\n",
      "Requirement already satisfied: fsspec in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from datasets->sagemaker-huggingface==0.0.1) (0.8.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.19.0->datasets->sagemaker-huggingface==0.0.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.19.0->datasets->sagemaker-huggingface==0.0.1) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from requests>=2.19.0->datasets->sagemaker-huggingface==0.0.1) (2020.12.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from matplotlib->sagemaker-huggingface==0.0.1) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from matplotlib->sagemaker-huggingface==0.0.1) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from matplotlib->sagemaker-huggingface==0.0.1) (8.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from matplotlib->sagemaker-huggingface==0.0.1) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from pandas->datasets->sagemaker-huggingface==0.0.1) (2020.4)\n",
      "Requirement already satisfied: google-pasta in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker->sagemaker-huggingface==0.0.1) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker->sagemaker-huggingface==0.0.1) (3.14.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker->sagemaker-huggingface==0.0.1) (20.8)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker->sagemaker-huggingface==0.0.1) (1.0.1)\n",
      "Requirement already satisfied: attrs in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker->sagemaker-huggingface==0.0.1) (20.3.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker->sagemaker-huggingface==0.0.1) (0.1.5)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker->sagemaker-huggingface==0.0.1) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from importlib-metadata>=1.4.0->sagemaker->sagemaker-huggingface==0.0.1) (3.4.0)\n",
      "Requirement already satisfied: docker-compose>=1.25.2 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker->sagemaker-huggingface==0.0.1) (1.27.4)\n",
      "Requirement already satisfied: PyYAML<6,>=5.3 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sagemaker->sagemaker-huggingface==0.0.1) (5.3.1)\n",
      "Requirement already satisfied: websocket-client<1,>=0.32.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (0.57.0)\n",
      "Requirement already satisfied: docker[ssh]<5,>=4.3.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (4.4.1)\n",
      "Requirement already satisfied: docopt<1,>=0.6.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (0.6.2)\n",
      "Requirement already satisfied: python-dotenv<1,>=0.13.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (0.15.0)\n",
      "Requirement already satisfied: jsonschema<4,>=2.5.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (3.2.0)\n",
      "Requirement already satisfied: texttable<2,>=0.9.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (1.6.3)\n",
      "Requirement already satisfied: cached-property<2,>=1.2.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (1.5.2)\n",
      "Requirement already satisfied: distro<2,>=1.5.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (1.5.0)\n",
      "Requirement already satisfied: dockerpty<1,>=0.4.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (0.4.1)\n",
      "Requirement already satisfied: paramiko>=2.4.2 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (2.7.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from jsonschema<4,>=2.5.1->docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (0.17.3)\n",
      "Requirement already satisfied: setuptools in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from jsonschema<4,>=2.5.1->docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (51.0.0.post20201207)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from paramiko>=2.4.2->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (3.3.1)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from paramiko>=2.4.2->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (3.2.0)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from paramiko>=2.4.2->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (1.4.0)\n",
      "Requirement already satisfied: cffi>=1.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from bcrypt>=3.1.3->paramiko>=2.4.2->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (1.14.4)\n",
      "Requirement already satisfied: pycparser in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4.2->docker[ssh]<5,>=4.3.1->docker-compose>=1.25.2->sagemaker->sagemaker-huggingface==0.0.1) (2.20)\n",
      "Requirement already satisfied: scikit-learn in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sklearn->sagemaker-huggingface==0.0.1) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from scikit-learn->sklearn->sagemaker-huggingface==0.0.1) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from scikit-learn->sklearn->sagemaker-huggingface==0.0.1) (1.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from scikit-learn->sklearn->sagemaker-huggingface==0.0.1) (1.5.4)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from transformers->sagemaker-huggingface==0.0.1) (0.9.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from transformers->sagemaker-huggingface==0.0.1) (2020.11.13)\n",
      "Requirement already satisfied: sacremoses in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from transformers->sagemaker-huggingface==0.0.1) (0.0.43)\n",
      "Requirement already satisfied: filelock in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from transformers->sagemaker-huggingface==0.0.1) (3.0.12)\n",
      "Requirement already satisfied: click in /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages (from sacremoses->transformers->sagemaker-huggingface==0.0.1) (7.1.2)\n",
      "Building wheels for collected packages: sagemaker-huggingface\n",
      "  Building wheel for sagemaker-huggingface (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker-huggingface: filename=sagemaker_huggingface-0.0.1-py3-none-any.whl size=16169 sha256=8dd3920ba03e343a431929394b9a92403199471fef51b06c4b492b9944d64ec1\n",
      "  Stored in directory: /private/var/folders/jj/dzns9hc55db1vmfsjvrh9n8m0000gp/T/pip-ephem-wheel-cache-ol73rt6e/wheels/ea/f4/50/478c4ff02760c480780476a589fe276064c446bc710087db4c\n",
      "Successfully built sagemaker-huggingface\n",
      "Installing collected packages: sagemaker-huggingface\n",
      "Successfully installed sagemaker-huggingface-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/philschmid/sagemaker-sdk-huggingface.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sagemaker-huggingface\r\n",
      "Version: 0.0.1\r\n",
      "Summary: Custom Sdk implementation for the huggingface libraries\r\n",
      "Home-page: https://github.com/philschmid/sagemaker-sdk-huggingface/tree/SagemakerTrainer\r\n",
      "Author: Philipp\r\n",
      "Author-email: schmidphilipp1995@gmail.com\r\n",
      "License: Apache 2.0\r\n",
      "Location: /Users/philippschmid/.anaconda3/envs/hf/lib/python3.8/site-packages\r\n",
      "Requires: sagemaker, sagemaker-experiments, datasets, sagemaker, torch, sklearn, boto3, numpy, transformers, matplotlib\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show sagemaker-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Sagemaker Session with local AWS Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From outside these notebooks, `get_execution_role()` will return an exception because it does not know what is the role name that SageMaker requires.\n",
    "\n",
    "To solve this issue, pass the IAM role name instead of using `get_execution_role()`.\n",
    "\n",
    "Therefore you have to create an IAM-Role with correct permission for sagemaker to start training jobs and download files from s3. Beware that you need s3 permission on bucket-level `\"arn:aws:s3:::sagemaker-*\"` and on object-level     `\"arn:aws:s3:::sagemaker-*/*\"`. \n",
    "\n",
    "You can read [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) how to create a role with right permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local aws profile configured in ~/.aws/credentials\n",
    "local_profile_name='hf-sm' # optional if you only have default configured\n",
    "\n",
    "# role name for sagemaker -> needs the described permissions from above\n",
    "role_name = \"SageMakerRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philipp to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::558105141721:role/SageMakerRole\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "try:\n",
    "    sess = sagemaker.Session()\n",
    "    role = sagemaker.get_execution_role()\n",
    "except Exception:\n",
    "    import boto3\n",
    "    # creates a boto3 session using the local profile we defined\n",
    "    if local_profile_name:\n",
    "        os.environ['AWS_PROFILE'] = local_profile_name # setting env var bc local-mode cannot use boto3 session\n",
    "        #bt3 = boto3.session.Session(profile_name=local_profile_name)\n",
    "        #iam = bt3.client('iam')\n",
    "        # create sagemaker session with boto3 session\n",
    "        #sess = sagemaker.Session(boto_session=bt3)\n",
    "    iam = boto3.client('iam')\n",
    "    sess = sagemaker.Session()\n",
    "    # get role arn\n",
    "    role = iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    \n",
    "\n",
    "\n",
    "print(role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Session prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/imdb/small/test/dataset.arrow', 'datasets/imdb/small/test/dataset_info.json', 'datasets/imdb/small/test/state.json', 'datasets/imdb/small/test/test_dataset.pt', 'datasets/imdb/small/train/dataset.arrow', 'datasets/imdb/small/train/dataset_info.json', 'datasets/imdb/small/train/state.json', 'datasets/imdb/small/training/train_dataset.pt']\n",
      "sagemaker-eu-central-1-558105141721\n",
      "eu-central-1\n"
     ]
    }
   ],
   "source": [
    "print(sess.list_s3_files(sess.default_bucket(),'datasets/')) # list objects in s3 under datsets/\n",
    "print(sess.default_bucket()) # s3 bucketname\n",
    "print(sess.boto_region_name) # aws region of sagemaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Since we are using the `.py` module directly from `huggingface/` we have to adjust our `sys.path` to be able to import our estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "Reusing dataset imdb (/Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "Loading cached shuffled indices for dataset at /Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-f7ed38da5ada7a37.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863cd3004eb148cb97049a6bf877e35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c88b6319844ddea77b65e4332961ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#helper tokenizer function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "# sample a to small dataset for training\n",
    "#train_dataset = train_dataset.shuffle().select(range(2000)) # smaller the size for test dataset to 10k \n",
    "#test_dataset = test_dataset.shuffle().select(range(150)) # smaller the size for test dataset to 10k \n",
    "\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset.rename_column_(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.rename_column_(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to sagemaker S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def upload_data_to_s3(dataset=None,prefix='datasets',split_type='train'):\n",
    "    \"\"\"helper function with saves the dataset locally using dataset.save_to_disk() and upload its then to s3. \"\"\"\n",
    "    \n",
    "    temp_prefix =f\"{prefix}/{split_type}\"\n",
    "    # saves datasets in local directory\n",
    "    dataset.save_to_disk(f\"./{temp_prefix}\")\n",
    "    \n",
    "    # loops over saved files and uploads them to s3 \n",
    "    for file in glob.glob(f\"./{temp_prefix}/*\"):\n",
    "        sess.upload_data(file, key_prefix=temp_prefix)\n",
    "\n",
    "    # return s3 url to files for estimator.fit()\n",
    "    return f\"s3://{sess.default_bucket()}/{temp_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-central-1-558105141721/datasets/imdb/train\n",
      "s3://sagemaker-eu-central-1-558105141721/datasets/imdb/test\n"
     ]
    }
   ],
   "source": [
    "prefix = 'datasets/imdb'\n",
    "\n",
    "training_input_path  = upload_data_to_s3(dataset=train_dataset,prefix=prefix,split_type='train')\n",
    "test_input_path      = upload_data_to_s3(dataset=test_dataset,prefix=prefix,split_type='test')\n",
    "\n",
    "print(training_input_path)\n",
    "print(test_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an local estimator for testing\n",
    "\n",
    "You run PyTorch training scripts on SageMaker by creating PyTorch Estimators. SageMaker training of your script is invoked when you call fit on a PyTorch Estimator. The following code sample shows how you train a custom PyTorch script `train.py`, passing in three hyperparameters (`epochs`). We are not going to pass any data into sagemaker training job instead it will be downloaded in `train.py`\n",
    "\n",
    "in sagemaker you can test you training in a \"local-mode\" by setting your instance_type to `'local'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msklearn\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmetrics\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m accuracy_score, precision_recall_fscore_support\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m ==\u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--eval-batch-size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output-data-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \u001b[36mprint\u001b[39;49;00m(args.training_dir)\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "    \u001b[36mprint\u001b[39;49;00m(glob.glob(args.training_dir+\u001b[33m\"\u001b[39;49;00m\u001b[33m/*\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))\n",
      "    \u001b[37m# Get datasets\u001b[39;49;00m\n",
      "    train_dataset  = torch.load(os.path.join(args.training_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtrain_dataset.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    test_dataset  = torch.load(os.path.join(args.test_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtest_dataset.pt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(train_dataset))\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(test_dataset))\n",
      "\n",
      "    \u001b[37m# compute metrics function for binary classification\u001b[39;49;00m\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(pred):\n",
      "        labels = pred.label_ids\n",
      "        preds = pred.predictions.argmax(-\u001b[34m1\u001b[39;49;00m)\n",
      "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\u001b[33m'\u001b[39;49;00m\u001b[33mbinary\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "        acc = accuracy_score(labels, preds)\n",
      "        \u001b[34mreturn\u001b[39;49;00m {\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: acc,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mf1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: f1,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mprecision\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: precision,\n",
      "            \u001b[33m'\u001b[39;49;00m\u001b[33mrecall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: recall\n",
      "        }\n",
      "\n",
      "    \u001b[37m# define training args \u001b[39;49;00m\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=args.model_dir,\n",
      "        num_train_epochs=args.epochs,\n",
      "        per_device_train_batch_size=args.train_batch_size,\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\n",
      "        warmup_steps=args.warmup_steps,\n",
      "        evaluation_strategy=\u001b[33m'\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        compute_metrics=compute_metrics,\n",
      "        train_dataset=train_dataset,\n",
      "        eval_dataset=test_dataset\n",
      "    )\n",
      "\n",
      "    \u001b[37m# train model\u001b[39;49;00m\n",
      "    trainer.train()\n",
      "\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
      "\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "     \u001b[37m# Saves the model to s3\u001b[39;49;00m\n",
      "    trainer.save_model(args.model_dir) \n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing custom sdk-extension for HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from huggingface.estimator import HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an local Estimator\n",
    "\n",
    "The following code sample shows how you train a custom HuggingFace script `train.py`, passing in three hyperparameters (`epochs`,`train_batch_size`,`model_name`). We are not going to pass any data into sagemaker training job instead it will be downloaded in `train.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            base_job_name='huggingface',\n",
    "                            instance_type='local',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version={'transformers':'4.1.1','datasets':'1.1.3'},\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'558105141721.dkr.ecr.eu-central-1.amazonaws.com/huggingface-training:0.0.1-cpu-transformers4.1.1-datasets1.1.3'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vjrxuebgqp-algo-1-7vnys ... \n",
      "\u001b[1BAttaching to vjrxuebgqp-algo-1-7vnys2mdone\u001b[0m\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:29,768 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:29,771 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:29,789 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:29,803 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:29,966 botocore.credentials INFO     Found credentials in environment variables.\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:30,238 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:30,259 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:30,278 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:30,294 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m \n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m Training Env:\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m \n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m {\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         \"train\": \"/opt/ml/input/data/train\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         \"test\": \"/opt/ml/input/data/test\"\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     },\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"current_host\": \"algo-1-7vnys\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         \"algo-1-7vnys\"\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     ],\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         \"epochs\": 1,\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         \"train_batch_size\": 32,\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         \"model_name\": \"distilbert-base-uncased\"\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     },\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         \"train\": {\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         },\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         \"test\": {\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         }\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     },\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"job_name\": \"huggingface-2020-12-27-17-08-21-833\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"master_hostname\": \"algo-1-7vnys\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"module_dir\": \"s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-27-17-08-21-833/source/sourcedir.tar.gz\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"module_name\": \"train\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"num_cpus\": 4,\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"num_gpus\": 0,\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         \"current_host\": \"algo-1-7vnys\",\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m             \"algo-1-7vnys\"\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m         ]\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     },\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m     \"user_entry_point\": \"train.py\"\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m }\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m \n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m Environment variables:\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m \n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_HOSTS=[\"algo-1-7vnys\"]\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_USER_ENTRY_POINT=train.py\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-7vnys\",\"hosts\":[\"algo-1-7vnys\"]}\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_INPUT_DATA_CONFIG={\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_CHANNELS=[\"test\",\"train\"]\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_CURRENT_HOST=algo-1-7vnys\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_MODULE_NAME=train\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_NUM_CPUS=4\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_NUM_GPUS=0\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_MODULE_DIR=s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-27-17-08-21-833/source/sourcedir.tar.gz\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1-7vnys\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-7vnys\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"TrainingInputMode\":\"File\"},\"train\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-2020-12-27-17-08-21-833\",\"log_level\":20,\"master_hostname\":\"algo-1-7vnys\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-558105141721/huggingface-2020-12-27-17-08-21-833/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-7vnys\",\"hosts\":[\"algo-1-7vnys\"]},\"user_entry_point\":\"train.py\"}\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_HP_EPOCHS=1\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_HP_TRAIN_BATCH_SIZE=32\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m SM_HP_MODEL_NAME=distilbert-base-uncased\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m \n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m \n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m /opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m \n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:33,574 - __main__ - INFO -  loaded train_dataset length is: 2000\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:33,574 - __main__ - INFO -  loaded test_dataset length is: 150\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:33,938 - filelock - INFO - Lock 140437597566176 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:34,280 - filelock - INFO - Lock 140437597566176 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:34,615 - filelock - INFO - Lock 140437597568248 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:08:48,068 - filelock - INFO - Lock 140437597568248 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\n",
      "Downloading: 100%|| 442/442 [00:00<00:00, 297kB/s]\n",
      "Downloading: 100%|| 268M/268M [00:13<00:00, 20.2MB/s] \n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /codebuild/output/src062189915/src/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m   return torch.tensor(x, **format_kwargs)\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m \n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m 2020-12-27 17:09:11,557 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m Command \"/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\"\n",
      "Downloading: 100%|| 442/442 [00:00<00:00, 297kB/s]\n",
      "Downloading: 100%|| 268M/268M [00:13<00:00, 20.2MB/s]\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m - This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m - This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  0%|          | 0/63 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /codebuild/output/src062189915/src/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys |\u001b[0m   return torch.tensor(x, **format_kwargs)\n",
      "\u001b[36mvjrxuebgqp-algo-1-7vnys exited with code 1\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run: ['docker-compose', '-f', '/private/var/folders/jj/dzns9hc55db1vmfsjvrh9n8m0000gp/T/tmp12pz5tan/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process exited with code: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Process exited with code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-e7ad92514db5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \"\"\"\n\u001b[1;32m   1418\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         self.model_artifacts = self.container.train(\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         )\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0;31m# which contains the exit code and append the command line to it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Failed to run: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompose_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0martifacts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve_artifacts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompose_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run: ['docker-compose', '-f', '/private/var/folders/jj/dzns9hc55db1vmfsjvrh9n8m0000gp/T/tmp12pz5tan/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Estimator\n",
    "\n",
    "The following code sample shows how you train a custom HuggingFace script `train.py`, passing in three hyperparameters (`epochs`,`train_batch_size`,`model_name`). We are not going to pass any data into sagemaker training job instead it will be downloaded in `train.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface.estimator import HuggingFace\n",
    "\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='../scripts',\n",
    "                            sagemaker_session=sess,\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version={'transformers':'4.1.1','datasets':'1.1.3'},\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimator Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get S3 url for model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-27-15-25-50-506/output/model.tar.gz'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get latest training job name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface-sdk-extension-2020-12-27-15-25-50-506'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.latest_training_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attach to old estimator \n",
    "\n",
    "e.g. to get model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_job_name='huggingface-sdk-extension-2020-12-27-15-25-50-506'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-12-27 15:34:03 Starting - Preparing the instances for training\n",
      "2020-12-27 15:34:03 Downloading - Downloading input data\n",
      "2020-12-27 15:34:03 Training - Training image download completed. Training in progress.\n",
      "2020-12-27 15:34:03 Uploading - Uploading generated training model\n",
      "2020-12-27 15:34:03 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator_loaded = Estimator.attach(old_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-27-15-25-50-506/output/model.tar.gz'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator_loaded.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download model from s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using huggingface utils**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface.utils import download_model\n",
    "\n",
    "download_model(model_data=huggingface_estimator_loaded.model_data,\n",
    "               unzip=True,\n",
    "               model_dir=huggingface_estimator_loaded.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**using class built-in method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.download_model(unzip=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access logs\n",
    "\n",
    "until [PR](https://github.com/aws/sagemaker-python-sdk/pull/2059) is merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-27 15:34:03 Starting - Preparing the instances for training\n",
      "2020-12-27 15:34:03 Downloading - Downloading input data\n",
      "2020-12-27 15:34:03 Training - Training image download completed. Training in progress.\n",
      "2020-12-27 15:34:03 Uploading - Uploading generated training model\n",
      "2020-12-27 15:34:03 Completed - Training job completed\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-12-27 15:32:15,198 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-27 15:32:15,221 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-12-27 15:32:18,249 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-27 15:32:18,802 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 32,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-sdk-extension-2020-12-27-15-25-50-506\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-27-15-25-50-506/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-27-15-25-50-506/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-sdk-extension-2020-12-27-15-25-50-506\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-27-15-25-50-506/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-27 15:32:22,212 - __main__ - INFO -  loaded train_dataset length is: 2000\u001b[0m\n",
      "\u001b[34m2020-12-27 15:32:22,213 - __main__ - INFO -  loaded test_dataset length is: 150\u001b[0m\n",
      "\u001b[34m2020-12-27 15:32:22,511 - filelock - INFO - Lock 140208028323288 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2020-12-27 15:32:22,796 - filelock - INFO - Lock 140208028323288 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2020-12-27 15:32:23,079 - filelock - INFO - Lock 140208028162648 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2020-12-27 15:32:28,027 - filelock - INFO - Lock 140208028162648 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[2020-12-27 15:32:33.312 algo-1:25 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-12-27 15:32:33.312 algo-1:25 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-12-27 15:32:33.312 algo-1:25 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-12-27 15:32:33.312 algo-1:25 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-12-27 15:32:33.329 algo-1:25 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-12-27 15:32:33.329 algo-1:25 INFO hook.py:461] Hook is writing from the hook with pid: 25\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-12-27 15:32:34.658 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2020-12-27 15:32:34.659 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2020-12-27 15:32:34.752 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.6371354460716248, 'eval_accuracy': 0.8533333333333334, 'eval_f1': 0.8253968253968254, 'eval_precision': 0.8666666666666667, 'eval_recall': 0.7878787878787878, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]#015Downloading: 100%|| 442/442 [00:00<00:00, 429kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   2%|         | 4.28M/268M [00:00<00:06, 42.8MB/s]#015Downloading:   3%|         | 9.31M/268M [00:00<00:05, 44.8MB/s]#015Downloading:   5%|         | 14.6M/268M [00:00<00:05, 47.1MB/s]#015Downloading:   7%|         | 20.0M/268M [00:00<00:05, 48.9MB/s]#015Downloading:  10%|         | 25.5M/268M [00:00<00:04, 50.6MB/s]#015Downloading:  12%|        | 31.0M/268M [00:00<00:04, 51.8MB/s]#015Downloading:  14%|        | 36.5M/268M [00:00<00:04, 52.8MB/s]#015Downloading:  16%|        | 42.0M/268M [00:00<00:04, 53.3MB/s]#015Downloading:  18%|        | 47.5M/268M [00:00<00:04, 53.9MB/s]#015Downloading:  20%|        | 53.1M/268M [00:01<00:03, 54.5MB/s]#015Downloading:  22%|       | 58.7M/268M [00:01<00:03, 54.9MB/s]#015Downloading:  24%|       | 64.4M/268M [00:01<00:03, 55.4MB/s]#015Downloading:  26%|       | 70.0M/268M [00:01<00:03, 55.7MB/s]#015Downloading:  28%|       | 75.6M/268M [00:01<00:03, 55.8MB/s]#015Downloading:  30%|       | 81.3M/268M [00:01<00:03, 56.1MB/s]#015Downloading:  32%|      | 86.9M/268M [00:01<00:03, 56.0MB/s]#015Downloading:  35%|      | 92.6M/268M [00:01<00:03, 56.2MB/s]#015Downloading:  37%|      | 98.2M/268M [00:01<00:03, 55.6MB/s]#015Downloading:  39%|      | 104M/268M [00:01<00:02, 55.5MB/s] #015Downloading:  41%|      | 109M/268M [00:02<00:02, 55.4MB/s]#015Downloading:  43%|     | 115M/268M [00:02<00:02, 52.6MB/s]#015Downloading:  45%|     | 120M/268M [00:02<00:02, 53.5MB/s]#015Downloading:  47%|     | 126M/268M [00:02<00:02, 54.3MB/s]#015Downloading:  49%|     | 132M/268M [00:02<00:02, 54.8MB/s]#015Downloading:  51%|     | 137M/268M [00:02<00:02, 55.2MB/s]#015Downloading:  53%|    | 143M/268M [00:02<00:02, 55.6MB/s]#015Downloading:  55%|    | 149M/268M [00:02<00:02, 55.8MB/s]#015Downloading:  58%|    | 154M/268M [00:02<00:02, 55.2MB/s]#015Downloading:  60%|    | 160M/268M [00:02<00:01, 55.4MB/s]#015Downloading:  62%|   | 165M/268M [00:03<00:01, 55.8MB/s]#015Downloading:  64%|   | 171M/268M [00:03<00:01, 55.9MB/s]#015Downloading:  66%|   | 177M/268M [00:03<00:01, 56.0MB/s]#015Downloading:  68%|   | 182M/268M [00:03<00:01, 56.1MB/s]#015Downloading:  70%|   | 188M/268M [00:03<00:01, 56.0MB/s]#015Downloading:  72%|  | 193M/268M [00:03<00:01, 56.2MB/s]#015Downloading:  74%|  | 199M/268M [00:03<00:01, 56.1MB/s]#015Downloading:  76%|  | 205M/268M [00:03<00:01, 56.3MB/s]#015Downloading:  79%|  | 210M/268M [00:03<00:01, 55.8MB/s]#015Downloading:  81%|  | 216M/268M [00:03<00:00, 55.9MB/s]#015Downloading:  83%| | 222M/268M [00:04<00:00, 56.0MB/s]#015Downloading:  85%| | 227M/268M [00:04<00:00, 55.9MB/s]#015Downloading:  87%| | 233M/268M [00:04<00:00, 55.9MB/s]#015Downloading:  89%| | 238M/268M [00:04<00:00, 56.0MB/s]#015Downloading:  91%| | 244M/268M [00:04<00:00, 55.8MB/s]#015Downloading:  93%|| 250M/268M [00:04<00:00, 55.9MB/s]#015Downloading:  95%|| 255M/268M [00:04<00:00, 56.0MB/s]#015Downloading:  97%|| 261M/268M [00:04<00:00, 56.1MB/s]#015Downloading:  99%|| 267M/268M [00:04<00:00, 55.6MB/s]#015Downloading: 100%|| 268M/268M [00:04<00:00, 55.1MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/63 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /codebuild/output/src926819546/src/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\u001b[0m\n",
      "\u001b[34m#015  2%|         | 1/63 [00:02<02:10,  2.10s/it]#015  3%|         | 2/63 [00:02<01:39,  1.62s/it]#015  5%|         | 3/63 [00:03<01:17,  1.29s/it]#015  6%|         | 4/63 [00:03<01:02,  1.05s/it]#015  8%|         | 5/63 [00:04<00:51,  1.13it/s]#015 10%|         | 6/63 [00:04<00:44,  1.29it/s]#015 11%|         | 7/63 [00:05<00:38,  1.45it/s]#015 13%|        | 8/63 [00:05<00:34,  1.58it/s]#015 14%|        | 9/63 [00:06<00:32,  1.68it/s]#015 16%|        | 10/63 [00:06<00:30,  1.76it/s]#015 17%|        | 11/63 [00:07<00:28,  1.82it/s]#015 19%|        | 12/63 [00:07<00:27,  1.87it/s]#015 21%|        | 13/63 [00:08<00:26,  1.90it/s]#015 22%|       | 14/63 [00:08<00:25,  1.93it/s]#015 24%|       | 15/63 [00:09<00:24,  1.94it/s]#015 25%|       | 16/63 [00:09<00:24,  1.96it/s]#015 27%|       | 17/63 [00:10<00:23,  1.97it/s]#015 29%|       | 18/63 [00:10<00:22,  1.98it/s]#015 30%|       | 19/63 [00:11<00:22,  1.98it/s]#015 32%|      | 20/63 [00:11<00:21,  1.98it/s]#015 33%|      | 21/63 [00:12<00:21,  1.98it/s]#015 35%|      | 22/63 [00:12<00:20,  1.99it/s]#015 37%|      | 23/63 [00:13<00:20,  1.99it/s]#015 38%|      | 24/63 [00:13<00:19,  1.99it/s]#015 40%|      | 25/63 [00:14<00:19,  1.99it/s]#015 41%|     | 26/63 [00:14<00:18,  1.99it/s]#015 43%|     | 27/63 [00:15<00:18,  1.99it/s]#015 44%|     | 28/63 [00:15<00:17,  1.99it/s]#015 46%|     | 29/63 [00:16<00:17,  1.99it/s]#015 48%|     | 30/63 [00:16<00:16,  1.99it/s]#015 49%|     | 31/63 [00:17<00:16,  1.98it/s]#015 51%|     | 32/63 [00:17<00:15,  1.98it/s]#015 52%|    | 33/63 [00:18<00:15,  1.98it/s]#015 54%|    | 34/63 [00:18<00:14,  1.97it/s]#015 56%|    | 35/63 [00:19<00:14,  1.98it/s]#015 57%|    | 36/63 [00:19<00:13,  1.98it/s]#015 59%|    | 37/63 [00:20<00:13,  1.97it/s]#015 60%|    | 38/63 [00:20<00:12,  1.98it/s]#015 62%|   | 39/63 [00:21<00:12,  1.94it/s]#015 63%|   | 40/63 [00:21<00:11,  1.95it/s]#015 65%|   | 41/63 [00:22<00:11,  1.96it/s]#015 67%|   | 42/63 [00:22<00:10,  1.97it/s]#015 68%|   | 43/63 [00:23<00:10,  1.98it/s]#015 70%|   | 44/63 [00:23<00:09,  1.98it/s]#015 71%|  | 45/63 [00:24<00:09,  1.98it/s]#015 73%|  | 46/63 [00:24<00:08,  1.98it/s]#015 75%|  | 47/63 [00:25<00:08,  1.98it/s]#015 76%|  | 48/63 [00:25<00:07,  1.99it/s]#015 78%|  | 49/63 [00:26<00:07,  1.98it/s]#015 79%|  | 50/63 [00:26<00:06,  1.98it/s]#015 81%|  | 51/63 [00:27<00:06,  1.98it/s]#015 83%| | 52/63 [00:27<00:05,  1.98it/s]#015 84%| | 53/63 [00:28<00:05,  1.97it/s]#015 86%| | 54/63 [00:28<00:04,  1.97it/s]#015 87%| | 55/63 [00:29<00:04,  1.97it/s]#015 89%| | 56/63 [00:29<00:03,  1.98it/s]#015 90%| | 57/63 [00:30<00:03,  1.98it/s]#015 92%|| 58/63 [00:30<00:02,  1.98it/s]#015 94%|| 59/63 [00:31<00:02,  1.98it/s]#015 95%|| 60/63 [00:31<00:01,  1.99it/s]#015 97%|| 61/63 [00:32<00:01,  1.98it/s]#015 98%|| 62/63 [00:32<00:00,  1.98it/s]#015100%|| 63/63 [00:33<00:00,  2.30it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|   | 2/3 [00:00<00:00,  7.28it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|| 3/3 [00:00<00:00,  5.59it/s]#033[A#015                                               #015\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015100%|| 63/63 [00:33<00:00,  2.30it/s]\u001b[0m\n",
      "\u001b[34m#015100%|| 3/3 [00:00<00:00,  5.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015                                               #015#015100%|| 63/63 [00:33<00:00,  2.30it/s]#015100%|| 63/63 [00:33<00:00,  1.86it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]#015 67%|   | 2/3 [00:00<00:00,  7.23it/s]#015100%|| 3/3 [00:00<00:00,  5.55it/s]#015100%|| 3/3 [00:00<00:00,  4.65it/s]\u001b[0m\n",
      "\u001b[34m2020-12-27 15:33:08,975 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[0m\n",
      "Training seconds: 316\n",
      "Billable seconds: 95\n",
      "Managed Spot Training savings: 69.9%\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**after merged PR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_estimator.logs()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
