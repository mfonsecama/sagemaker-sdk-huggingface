{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk extension example using `Trainer` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs requirements if you haven´t already done it and sets up ipywidgets for datasets in sagemaker studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os \n",
    "import IPython\n",
    "if 'SAGEMAKER_TRAINING_MODULE' in os.environ:\n",
    "    !conda install -c conda-forge ipywidgets -y\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Sagemaker Session with local AWS Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From outside these notebooks, `get_execution_role()` will return an exception because it does not know what is the role name that SageMaker requires.\n",
    "\n",
    "To solve this issue, pass the IAM role name instead of using `get_execution_role()`.\n",
    "\n",
    "Therefore you have to create an IAM-Role with correct permission for sagemaker to start training jobs and download files from s3. Beware that you need s3 permission on bucket-level `\"arn:aws:s3:::sagemaker-*\"` and on object-level     `\"arn:aws:s3:::sagemaker-*/*\"`. \n",
    "\n",
    "You can read [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) how to create a role with right permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local aws profile configured in ~/.aws/credentials\n",
    "local_profile_name='hf-sm' # optional if you only have default configured\n",
    "\n",
    "# role name for sagemaker -> needs the described permissions from above\n",
    "role_name = \"SageMakerRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "WARNING:sagemaker:Couldn't call 'get_role' to get Role ARN from role name philipp to get Role path.\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::558105141721:role/SageMakerRole\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "try:\n",
    "    sess = sagemaker.Session()\n",
    "    role = sagemaker.get_execution_role()\n",
    "except Exception:\n",
    "    import boto3\n",
    "    # creates a boto3 session using the local profile we defined\n",
    "    if local_profile_name:\n",
    "        os.environ['AWS_PROFILE'] = local_profile_name # setting env var bc local-mode cannot use boto3 session\n",
    "        #bt3 = boto3.session.Session(profile_name=local_profile_name)\n",
    "        #iam = bt3.client('iam')\n",
    "        # create sagemaker session with boto3 session\n",
    "        #sess = sagemaker.Session(boto_session=bt3)\n",
    "    iam = boto3.client('iam')\n",
    "    sess = sagemaker.Session()\n",
    "    # get role arn\n",
    "    role = iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    \n",
    "\n",
    "\n",
    "print(role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Session prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/imdb/small/test/dataset.arrow', 'datasets/imdb/small/test/dataset_info.json', 'datasets/imdb/small/test/state.json', 'datasets/imdb/small/test/test_dataset.pt', 'datasets/imdb/small/train/dataset.arrow', 'datasets/imdb/small/train/dataset_info.json', 'datasets/imdb/small/train/state.json', 'datasets/imdb/small/training/train_dataset.pt', 'datasets/imdb/test/dataset.arrow', 'datasets/imdb/test/dataset_info.json', 'datasets/imdb/test/state.json', 'datasets/imdb/train/dataset.arrow', 'datasets/imdb/train/dataset_info.json', 'datasets/imdb/train/state.json']\n",
      "sagemaker-eu-central-1-558105141721\n",
      "eu-central-1\n"
     ]
    }
   ],
   "source": [
    "print(sess.list_s3_files(sess.default_bucket(),'datasets/')) # list objects in s3 under datsets/\n",
    "print(sess.default_bucket()) # s3 bucketname\n",
    "print(sess.boto_region_name) # aws region of sagemaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Since we are using the `.py` module directly from `huggingface/` we have to adjust our `sys.path` to be able to import our estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "Reusing dataset imdb (/Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "Loading cached shuffled indices for dataset at /Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-f7ed38da5ada7a37.arrow\n",
      "Loading cached processed dataset at /Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-700f1c95c213ac49.arrow\n",
      "Loading cached processed dataset at /Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-0cc657b0f33fcf97.arrow\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#helper tokenizer function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "# sample a to small dataset for training\n",
    "#train_dataset = train_dataset.shuffle().select(range(2000)) # smaller the size for test dataset to 10k \n",
    "#test_dataset = test_dataset.shuffle().select(range(150)) # smaller the size for test dataset to 10k \n",
    "\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset.rename_column_(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.rename_column_(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to sagemaker S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def upload_data_to_s3(dataset=None,prefix='datasets',split_type='train'):\n",
    "    \"\"\"helper function with saves the dataset locally using dataset.save_to_disk() and upload its then to s3. \"\"\"\n",
    "    \n",
    "    temp_prefix =f\"{prefix}/{split_type}\"\n",
    "    # saves datasets in local directory\n",
    "    dataset.save_to_disk(f\"./{temp_prefix}\")\n",
    "    \n",
    "    # loops over saved files and uploads them to s3 \n",
    "    for file in glob.glob(f\"./{temp_prefix}/*\"):\n",
    "        sess.upload_data(file, key_prefix=temp_prefix)\n",
    "\n",
    "    # return s3 url to files for estimator.fit()\n",
    "    return f\"s3://{sess.default_bucket()}/{temp_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-central-1-558105141721/datasets/imdb/train\n",
      "s3://sagemaker-eu-central-1-558105141721/datasets/imdb/test\n"
     ]
    }
   ],
   "source": [
    "prefix = 'datasets/imdb'\n",
    "\n",
    "training_input_path  = upload_data_to_s3(dataset=train_dataset,prefix=prefix,split_type='train')\n",
    "test_input_path      = upload_data_to_s3(dataset=test_dataset,prefix=prefix,split_type='test')\n",
    "\n",
    "print(training_input_path)\n",
    "print(test_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sagemaker Experiment for tracking](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html)\n",
    "\n",
    "Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare, and evaluate your machine learning experiments.\n",
    "\n",
    "SageMaker Experiments comes with its own Experiments SDK which makes the analytics capabilities easily accessible in Amazon SageMaker Notebooks. Because SageMaker Experiments enables tracking of all the steps and artifacts that went into creating a model, you can quickly revisit the origins of a model when you are troubleshooting issues in production, or auditing your models for compliance verifications.\n",
    "\n",
    "\n",
    "[Sagemaker Experiment docs](https://sagemaker-experiments.readthedocs.io/en/latest/)\n",
    "\n",
    "Sagemaker Experiment Components are:\n",
    "* Experiment\n",
    "* Trial\n",
    "* Trial Component\n",
    "* Tracker \n",
    "\n",
    "\n",
    "**Experiment:**  \n",
    "An Amazon SageMaker experiment is a collection of trials.\n",
    "\n",
    "**Trial**  \n",
    "An execution of a data-science workflow with an experiment.\n",
    "Consists of a list of trial component objects, which document individual activities within the workflow. An Trial_Component for example is a `TrainingJob`. If you run an `estimator.fit()` with a trial. The `estimator` will automatically log all hyperparameters and metrics to trial. \n",
    "\n",
    "**Trial Component:**  \n",
    "\n",
    "A trial component is a stage in a trial.\n",
    "Trial components are created automatically within the SageMaker runtime and may not be created directly. To automatically associate trial components with a trial and experiment supply an experiment config when creating a job. For example: https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateTrainingJob.html\n",
    "\n",
    "**Tracker:**\n",
    "A tracker records experiment information to a SageMaker trial component. (custom logger in `train.py`)\n",
    "When creating a tracker within a SageMaker training or processing job, use the load method with no arguments to track artifacts to the trial component automatically created for your job. When tracking within a Jupyternotebook running in SageMaker, use the create method to automatically create a new trial component.\n",
    "\n",
    "Trackers are Python context managers and you can use them using the Python with keyword. Exceptions thrown within the with block will cause the tracker’s trial component to be marked as failed. Start and end times are automatically set when using the with statement and the trial component is saved to SageMaker at the end of the block.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Create an Amazon SageMaker Experiment](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments-create.html)\n",
    "\n",
    "AWS provides an extra sdk to use `sagemaker-experiments`. If you haven´t already installed go a head an run. \n",
    "\n",
    "```python\n",
    "pip install sagemaker-experiments\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "#boto3\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create sagemaker session with correct aws profile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if local_profile_name:\n",
    "    os.environ['AWS_PROFILE'] = local_profile_name # setting env var bc local-mode cannot use boto3 session\n",
    "    \n",
    "ex_sm_sess = boto3.client('sagemaker')\n",
    "sess_experiment = Experiment(sagemaker_boto_client=ex_sm_sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create experiment and trial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "experiment = sess_experiment.create(experiment_name='hf-1') # ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,119}\n",
    "trial = experiment.create_trial(trial_name='text-classification') # ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,119}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**list experiments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExperimentSummary(experiment_name='hf-1',experiment_arn='arn:aws:sagemaker:eu-central-1:558105141721:experiment/hf-1',display_name='hf-1',creation_time=datetime.datetime(2020, 12, 28, 17, 44, 43, 808000, tzinfo=tzlocal()),last_modified_time=datetime.datetime(2020, 12, 28, 17, 44, 43, 867000, tzinfo=tzlocal()))\n",
      "TrialSummary(trial_name='text-classification',trial_arn='arn:aws:sagemaker:eu-central-1:558105141721:experiment-trial/text-classification',display_name='text-classification',creation_time=datetime.datetime(2020, 12, 28, 17, 44, 43, 867000, tzinfo=tzlocal()),last_modified_time=datetime.datetime(2020, 12, 28, 17, 44, 43, 867000, tzinfo=tzlocal()))\n"
     ]
    }
   ],
   "source": [
    "for exp in Experiment.list():\n",
    "    print(exp)\n",
    "for trial in experiment.list_trials():\n",
    "    print(trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**delete experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiment.delete_all(action=\"--force\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing custom sdk-extension for HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path  = 's3://sagemaker-eu-central-1-558105141721/datasets/imdb/small/train'\n",
    "test_input_path      = 's3://sagemaker-eu-central-1-558105141721/datasets/imdb/small/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'huggingface'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c7f173cd9da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'huggingface'"
     ]
    }
   ],
   "source": [
    "from huggingface.estimator import HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Estimator with an Experiment\n",
    "\n",
    "[Metric Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/training-metrics.html)\n",
    "\n",
    "To find a metric, SageMaker searches the logs that your algorithm emits and finds logs that match the regular expression that you specify for that metric. \n",
    "\n",
    "Defining Training Metrics (SageMaker Python SDK)\n",
    "Define the metrics that you want to send to CloudWatch by specifying a list of metric names and regular expressions as the metric_definitions argument when you initialize an Estimator object. For example, if you want to monitor both the train:error and validation:error metrics in CloudWatch, your Estimator initialization would look like the following:\n",
    "```python\n",
    "Estimator(\n",
    "    ...,\n",
    "    sagemaker_session = sm_sess,\n",
    "    tags = [{'Key': 'my-experiments', 'Value': 'demo2'}])\n",
    "\n",
    "estimator.fit(\n",
    "    ...,\n",
    "    experiment_config = {\n",
    "        # \"ExperimentName\"\n",
    "        \"TrialName\" : demo_trial.trial_name,\n",
    "        \"TrialComponentDisplayName\" : \"TrainingJob\",\n",
    "    })\n",
    "```\n",
    "\n",
    "In the regex for the train:error metric defined above, the first part of the regex finds the exact text `\"Train_error=\"`, and the expression `(.*?);` captures zero or more of any character until the first `;` semicolon character.\n",
    "\n",
    "For more information about training by using Amazon SageMaker Python SDK estimators, see https://github.com/aws/sagemaker-python-sdk#sagemaker-python-sdk-overview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface.estimator import HuggingFace\n",
    "\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='../scripts',\n",
    "                            sagemaker_session=sess,\n",
    "                            use_spot_instances=True,\n",
    "                            max_wait=4600, # This should be equal to or greater than max_run in seconds'\n",
    "                            max_run=3600,\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version={'transformers':'4.1.1','datasets':'1.1.3'},\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 3,\n",
    "                                               'train_batch_size': 16,\n",
    "                                               'model_name':'distilbert-base-uncased'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text-classification'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.trial_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'558105141721.dkr.ecr.eu-central-1.amazonaws.com/huggingface-training:0.0.1-gpu-transformers4.1.1-datasets1.1.3-cu110'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huggingface_estimator.image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: huggingface-sdk-extension-2020-12-28-16-53-59-249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-28 16:53:59 Starting - Starting the training job...\n",
      "2020-12-28 16:54:23 Starting - Launching requested ML instancesProfilerReport-1609174439: InProgress\n",
      "......\n",
      "2020-12-28 16:55:25 Starting - Preparing the instances for training......\n",
      "2020-12-28 16:56:31 Downloading - Downloading input data\n",
      "2020-12-28 16:56:31 Training - Downloading the training image.....................\n",
      "2020-12-28 16:59:55 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-12-28 16:59:56,073 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-28 16:59:56,097 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-12-28 16:59:57,522 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-28 16:59:57,992 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 16,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 3\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-sdk-extension-2020-12-28-16-53-59-249\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-28-16-53-59-249/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":16}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-28-16-53-59-249/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":16},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-sdk-extension-2020-12-28-16-53-59-249\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2020-12-28-16-53-59-249/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"16\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --epochs 3 --model_name distilbert-base-uncased --train_batch_size 16\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-28 17:00:01,593 - __main__ - INFO -  loaded train_dataset length is: 2000\u001b[0m\n",
      "\u001b[34m2020-12-28 17:00:01,593 - __main__ - INFO -  loaded test_dataset length is: 150\u001b[0m\n",
      "\u001b[34m2020-12-28 17:00:01,900 - filelock - INFO - Lock 140213814332216 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2020-12-28 17:00:02,186 - filelock - INFO - Lock 140213814332216 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2020-12-28 17:00:02,471 - filelock - INFO - Lock 140213813937040 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2020-12-28 17:00:07,577 - filelock - INFO - Lock 140213813937040 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[2020-12-28 17:00:12.870 algo-1:25 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-12-28 17:00:12.871 algo-1:25 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-12-28 17:00:12.871 algo-1:25 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-12-28 17:00:12.871 algo-1:25 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-12-28 17:00:12.888 algo-1:25 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-12-28 17:00:12.889 algo-1:25 INFO hook.py:461] Hook is writing from the hook with pid: 25\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-12-28 17:00:14.035 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2020-12-28 17:00:14.035 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2020-12-28 17:00:14.124 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m{'eval_loss': 0.6602308750152588, 'eval_accuracy': 0.68, 'eval_f1': 0.44186046511627913, 'eval_precision': 0.95, 'eval_recall': 0.2878787878787879, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.26919224858283997, 'eval_accuracy': 0.9066666666666666, 'eval_f1': 0.8955223880597014, 'eval_precision': 0.8823529411764706, 'eval_recall': 0.9090909090909091, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.3070515990257263, 'eval_accuracy': 0.88, 'eval_f1': 0.8524590163934426, 'eval_precision': 0.9285714285714286, 'eval_recall': 0.7878787878787878, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 442/442 [00:00<00:00, 425kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.65M/268M [00:00<00:05, 46.5MB/s]#015Downloading:   4%|▎         | 9.91M/268M [00:00<00:05, 48.2MB/s]#015Downloading:   6%|▌         | 15.7M/268M [00:00<00:04, 50.8MB/s]#015Downloading:   8%|▊         | 21.7M/268M [00:00<00:04, 53.2MB/s]#015Downloading:  10%|█         | 27.8M/268M [00:00<00:04, 55.2MB/s]#015Downloading:  12%|█▏        | 33.2M/268M [00:00<00:04, 55.0MB/s]#015Downloading:  14%|█▍        | 38.3M/268M [00:00<00:04, 53.7MB/s]#015Downloading:  16%|█▌        | 43.5M/268M [00:00<00:04, 53.1MB/s]#015Downloading:  18%|█▊        | 48.8M/268M [00:00<00:04, 53.1MB/s]#015Downloading:  20%|██        | 54.2M/268M [00:01<00:04, 53.3MB/s]#015Downloading:  22%|██▏       | 59.6M/268M [00:01<00:03, 53.4MB/s]#015Downloading:  24%|██▍       | 64.8M/268M [00:01<00:03, 52.9MB/s]#015Downloading:  26%|██▌       | 70.2M/268M [00:01<00:03, 53.1MB/s]#015Downloading:  28%|██▊       | 75.5M/268M [00:01<00:03, 53.0MB/s]#015Downloading:  30%|███       | 80.9M/268M [00:01<00:03, 53.3MB/s]#015Downloading:  32%|███▏      | 86.2M/268M [00:01<00:03, 52.7MB/s]#015Downloading:  34%|███▍      | 91.5M/268M [00:01<00:03, 52.9MB/s]#015Downloading:  36%|███▌      | 96.9M/268M [00:01<00:03, 53.2MB/s]#015Downloading:  38%|███▊      | 102M/268M [00:01<00:03, 53.2MB/s] #015Downloading:  40%|████      | 108M/268M [00:02<00:03, 53.3MB/s]#015Downloading:  42%|████▏     | 113M/268M [00:02<00:02, 53.3MB/s]#015Downloading:  44%|████▍     | 118M/268M [00:02<00:02, 52.9MB/s]#015Downloading:  46%|████▌     | 124M/268M [00:02<00:02, 52.8MB/s]#015Downloading:  48%|████▊     | 129M/268M [00:02<00:02, 52.8MB/s]#015Downloading:  50%|█████     | 134M/268M [00:02<00:02, 53.1MB/s]#015Downloading:  52%|█████▏    | 140M/268M [00:02<00:02, 53.3MB/s]#015Downloading:  54%|█████▍    | 145M/268M [00:02<00:02, 53.2MB/s]#015Downloading:  56%|█████▌    | 150M/268M [00:02<00:02, 53.3MB/s]#015Downloading:  58%|█████▊    | 156M/268M [00:02<00:02, 53.4MB/s]#015Downloading:  60%|██████    | 161M/268M [00:03<00:01, 53.5MB/s]#015Downloading:  62%|██████▏   | 166M/268M [00:03<00:01, 53.5MB/s]#015Downloading:  64%|██████▍   | 172M/268M [00:03<00:01, 53.0MB/s]#015Downloading:  66%|██████▌   | 177M/268M [00:03<00:01, 53.1MB/s]#015Downloading:  68%|██████▊   | 182M/268M [00:03<00:01, 53.0MB/s]#015Downloading:  70%|███████   | 188M/268M [00:03<00:01, 51.8MB/s]#015Downloading:  72%|███████▏  | 193M/268M [00:03<00:01, 51.4MB/s]#015Downloading:  74%|███████▍  | 198M/268M [00:03<00:01, 52.0MB/s]#015Downloading:  76%|███████▌  | 204M/268M [00:03<00:01, 52.6MB/s]#015Downloading:  78%|███████▊  | 209M/268M [00:03<00:01, 53.1MB/s]#015Downloading:  80%|████████  | 214M/268M [00:04<00:01, 53.3MB/s]#015Downloading:  82%|████████▏ | 220M/268M [00:04<00:00, 53.3MB/s]#015Downloading:  84%|████████▍ | 225M/268M [00:04<00:00, 53.1MB/s]#015Downloading:  86%|████████▌ | 230M/268M [00:04<00:00, 53.2MB/s]#015Downloading:  88%|████████▊ | 236M/268M [00:04<00:00, 53.4MB/s]#015Downloading:  90%|█████████ | 241M/268M [00:04<00:00, 53.6MB/s]#015Downloading:  92%|█████████▏| 247M/268M [00:04<00:00, 53.6MB/s]#015Downloading:  94%|█████████▍| 252M/268M [00:04<00:00, 52.6MB/s]#015Downloading:  96%|█████████▌| 257M/268M [00:04<00:00, 53.0MB/s]#015Downloading:  98%|█████████▊| 263M/268M [00:04<00:00, 53.3MB/s]#015Downloading: 100%|██████████| 268M/268M [00:05<00:00, 53.3MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/189 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /codebuild/output/src926819546/src/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\u001b[0m\n",
      "\u001b[34m#015  1%|          | 1/189 [00:01<06:02,  1.93s/it]#015  1%|          | 2/189 [00:02<04:40,  1.50s/it]#015  2%|▏         | 3/189 [00:02<03:43,  1.20s/it]#015  2%|▏         | 4/189 [00:03<03:04,  1.00it/s]#015  3%|▎         | 5/189 [00:03<02:36,  1.18it/s]#015  3%|▎         | 6/189 [00:04<02:16,  1.34it/s]#015  4%|▎         | 7/189 [00:04<02:02,  1.48it/s]#015  4%|▍         | 8/189 [00:05<01:53,  1.60it/s]#015  5%|▍         | 9/189 [00:06<01:46,  1.69it/s]#015  5%|▌         | 10/189 [00:06<01:41,  1.77it/s]#015  6%|▌         | 11/189 [00:07<01:37,  1.82it/s]#015  6%|▋         | 12/189 [00:07<01:35,  1.86it/s]#015  7%|▋         | 13/189 [00:08<01:32,  1.89it/s]#015  7%|▋         | 14/189 [00:08<01:31,  1.91it/s]#015  8%|▊         | 15/189 [00:09<01:30,  1.93it/s]#015  8%|▊         | 16/189 [00:09<01:29,  1.94it/s]#015  9%|▉         | 17/189 [00:10<01:28,  1.95it/s]#015 10%|▉         | 18/189 [00:10<01:27,  1.95it/s]#015 10%|█         | 19/189 [00:11<01:26,  1.96it/s]#015 11%|█         | 20/189 [00:11<01:26,  1.96it/s]#015 11%|█         | 21/189 [00:12<01:25,  1.96it/s]#015 12%|█▏        | 22/189 [00:12<01:25,  1.96it/s]#015 12%|█▏        | 23/189 [00:13<01:24,  1.96it/s]#015 13%|█▎        | 24/189 [00:13<01:24,  1.96it/s]#015 13%|█▎        | 25/189 [00:14<01:23,  1.96it/s]#015 14%|█▍        | 26/189 [00:14<01:23,  1.96it/s]#015 14%|█▍        | 27/189 [00:15<01:22,  1.96it/s]#015 15%|█▍        | 28/189 [00:15<01:22,  1.96it/s]#015 15%|█▌        | 29/189 [00:16<01:21,  1.96it/s]#015 16%|█▌        | 30/189 [00:16<01:21,  1.96it/s]#015 16%|█▋        | 31/189 [00:17<01:20,  1.96it/s]#015 17%|█▋        | 32/189 [00:17<01:20,  1.96it/s]#015 17%|█▋        | 33/189 [00:18<01:19,  1.96it/s]#015 18%|█▊        | 34/189 [00:18<01:19,  1.96it/s]#015 19%|█▊        | 35/189 [00:19<01:19,  1.95it/s]#015 19%|█▉        | 36/189 [00:19<01:18,  1.94it/s]#015 20%|█▉        | 37/189 [00:20<01:18,  1.95it/s]#015 20%|██        | 38/189 [00:20<01:17,  1.95it/s]#015 21%|██        | 39/189 [00:21<01:19,  1.88it/s]#015 21%|██        | 40/189 [00:21<01:18,  1.91it/s]#015 22%|██▏       | 41/189 [00:22<01:16,  1.92it/s]#015 22%|██▏       | 42/189 [00:22<01:16,  1.93it/s]#015 23%|██▎       | 43/189 [00:23<01:15,  1.94it/s]#015 23%|██▎       | 44/189 [00:23<01:14,  1.94it/s]#015 24%|██▍       | 45/189 [00:24<01:14,  1.94it/s]#015 24%|██▍       | 46/189 [00:24<01:13,  1.94it/s]#015 25%|██▍       | 47/189 [00:25<01:12,  1.95it/s]#015 25%|██▌       | 48/189 [00:25<01:12,  1.95it/s]#015 26%|██▌       | 49/189 [00:26<01:11,  1.95it/s]#015 26%|██▋       | 50/189 [00:27<01:11,  1.95it/s]#015 27%|██▋       | 51/189 [00:27<01:10,  1.95it/s]#015 28%|██▊       | 52/189 [00:28<01:10,  1.95it/s]#015 28%|██▊       | 53/189 [00:28<01:09,  1.95it/s]#015 29%|██▊       | 54/189 [00:29<01:09,  1.95it/s]#015 29%|██▉       | 55/189 [00:29<01:08,  1.95it/s]#015 30%|██▉       | 56/189 [00:30<01:08,  1.95it/s]#015 30%|███       | 57/189 [00:30<01:07,  1.95it/s]#015 31%|███       | 58/189 [00:31<01:07,  1.95it/s]#015 31%|███       | 59/189 [00:31<01:06,  1.94it/s]#015 32%|███▏      | 60/189 [00:32<01:06,  1.94it/s]#015 32%|███▏      | 61/189 [00:32<01:05,  1.94it/s]#015 33%|███▎      | 62/189 [00:33<01:05,  1.95it/s]#015 33%|███▎      | 63/189 [00:33<00:55,  2.26it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m2020-12-28 17:01:55,164 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 2/3 [00:00<00:00,  7.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.47it/s]#033[A#015                                                #015\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015 33%|███▎      | 63/189 [00:34<00:55,  2.26it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.47it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015 34%|███▍      | 64/189 [00:34<01:27,  1.44it/s]#015 34%|███▍      | 65/189 [00:35<01:19,  1.56it/s]#015 35%|███▍      | 66/189 [00:35<01:14,  1.65it/s]#015 35%|███▌      | 67/189 [00:36<01:10,  1.73it/s]#015 36%|███▌      | 68/189 [00:36<01:07,  1.79it/s]#015 37%|███▋      | 69/189 [00:37<01:05,  1.83it/s]#015 37%|███▋      | 70/189 [00:37<01:03,  1.87it/s]#015 38%|███▊      | 71/189 [00:38<01:02,  1.89it/s]#015 38%|███▊      | 72/189 [00:38<01:01,  1.91it/s]#015 39%|███▊      | 73/189 [00:39<01:00,  1.92it/s]#015 39%|███▉      | 74/189 [00:39<00:59,  1.93it/s]#015 40%|███▉      | 75/189 [00:40<00:58,  1.94it/s]#015 40%|████      | 76/189 [00:40<00:58,  1.94it/s]#015 41%|████      | 77/189 [00:41<00:57,  1.94it/s]#015 41%|████▏     | 78/189 [00:41<00:57,  1.93it/s]#015 42%|████▏     | 79/189 [00:42<00:56,  1.93it/s]#015 42%|████▏     | 80/189 [00:42<00:56,  1.94it/s]#015 43%|████▎     | 81/189 [00:43<00:55,  1.94it/s]#015 43%|████▎     | 82/189 [00:43<00:55,  1.94it/s]#015 44%|████▍     | 83/189 [00:44<00:54,  1.94it/s]#015 44%|████▍     | 84/189 [00:45<00:54,  1.94it/s]#015 45%|████▍     | 85/189 [00:45<00:53,  1.94it/s]#015 46%|████▌     | 86/189 [00:46<00:53,  1.94it/s]#015 46%|████▌     | 87/189 [00:46<00:52,  1.94it/s]#015 47%|████▋     | 88/189 [00:47<00:52,  1.94it/s]#015 47%|████▋     | 89/189 [00:47<00:51,  1.94it/s]#015 48%|████▊     | 90/189 [00:48<00:50,  1.94it/s]#015 48%|████▊     | 91/189 [00:48<00:50,  1.94it/s]#015 49%|████▊     | 92/189 [00:49<00:49,  1.94it/s]#015 49%|████▉     | 93/189 [00:49<00:49,  1.94it/s]#015 50%|████▉     | 94/189 [00:50<00:49,  1.94it/s]#015 50%|█████     | 95/189 [00:50<00:48,  1.93it/s]#015 51%|█████     | 96/189 [00:51<00:48,  1.94it/s]#015 51%|█████▏    | 97/189 [00:51<00:47,  1.94it/s]#015 52%|█████▏    | 98/189 [00:52<00:46,  1.94it/s]#015 52%|█████▏    | 99/189 [00:52<00:46,  1.94it/s]#015 53%|█████▎    | 100/189 [00:53<00:45,  1.94it/s]#015 53%|█████▎    | 101/189 [00:53<00:45,  1.94it/s]#015 54%|█████▍    | 102/189 [00:54<00:44,  1.93it/s]#015 54%|█████▍    | 103/189 [00:54<00:44,  1.94it/s]#015 55%|█████▌    | 104/189 [00:55<00:43,  1.94it/s]#015 56%|█████▌    | 105/189 [00:55<00:43,  1.93it/s]#015 56%|█████▌    | 106/189 [00:56<00:43,  1.93it/s]#015 57%|█████▋    | 107/189 [00:56<00:42,  1.93it/s]#015 57%|█████▋    | 108/189 [00:57<00:42,  1.93it/s]#015 58%|█████▊    | 109/189 [00:57<00:41,  1.93it/s]#015 58%|█████▊    | 110/189 [00:58<00:40,  1.93it/s]#015 59%|█████▊    | 111/189 [00:58<00:40,  1.93it/s]#015 59%|█████▉    | 112/189 [00:59<00:39,  1.94it/s]#015 60%|█████▉    | 113/189 [01:00<00:39,  1.94it/s]#015 60%|██████    | 114/189 [01:00<00:38,  1.94it/s]#015 61%|██████    | 115/189 [01:01<00:38,  1.93it/s]#015 61%|██████▏   | 116/189 [01:01<00:37,  1.94it/s]#015 62%|██████▏   | 117/189 [01:02<00:37,  1.94it/s]#015 62%|██████▏   | 118/189 [01:02<00:36,  1.94it/s]#015 63%|██████▎   | 119/189 [01:03<00:36,  1.94it/s]#015 63%|██████▎   | 120/189 [01:03<00:35,  1.94it/s]#015 64%|██████▍   | 121/189 [01:04<00:35,  1.94it/s]#015 65%|██████▍   | 122/189 [01:04<00:34,  1.94it/s]#015 65%|██████▌   | 123/189 [01:05<00:33,  1.94it/s]#015 66%|██████▌   | 124/189 [01:05<00:33,  1.94it/s]#015 66%|██████▌   | 125/189 [01:06<00:32,  1.94it/s]#015 67%|██████▋   | 126/189 [01:06<00:27,  2.26it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 2/3 [00:00<00:00,  7.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.39it/s]#033[A#015                                                 #015\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015 67%|██████▋   | 126/189 [01:07<00:27,  2.26it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.39it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015 67%|██████▋   | 127/189 [01:07<00:43,  1.43it/s]#015 68%|██████▊   | 128/189 [01:08<00:39,  1.55it/s]#015 68%|██████▊   | 129/189 [01:08<00:36,  1.64it/s]#015 69%|██████▉   | 130/189 [01:09<00:34,  1.72it/s]#015 69%|██████▉   | 131/189 [01:09<00:32,  1.78it/s]#015 70%|██████▉   | 132/189 [01:10<00:31,  1.82it/s]#015 70%|███████   | 133/189 [01:10<00:30,  1.86it/s]#015 71%|███████   | 134/189 [01:11<00:29,  1.88it/s]#015 71%|███████▏  | 135/189 [01:11<00:28,  1.90it/s]#015 72%|███████▏  | 136/189 [01:12<00:27,  1.91it/s]#015 72%|███████▏  | 137/189 [01:12<00:27,  1.91it/s]#015 73%|███████▎  | 138/189 [01:13<00:26,  1.92it/s]#015 74%|███████▎  | 139/189 [01:13<00:26,  1.92it/s]#015 74%|███████▍  | 140/189 [01:14<00:25,  1.92it/s]#015 75%|███████▍  | 141/189 [01:15<00:25,  1.92it/s]#015 75%|███████▌  | 142/189 [01:15<00:24,  1.92it/s]#015 76%|███████▌  | 143/189 [01:16<00:23,  1.92it/s]#015 76%|███████▌  | 144/189 [01:16<00:23,  1.92it/s]#015 77%|███████▋  | 145/189 [01:17<00:22,  1.92it/s]#015 77%|███████▋  | 146/189 [01:17<00:22,  1.92it/s]#015 78%|███████▊  | 147/189 [01:18<00:21,  1.92it/s]#015 78%|███████▊  | 148/189 [01:18<00:21,  1.92it/s]#015 79%|███████▉  | 149/189 [01:19<00:20,  1.93it/s]#015 79%|███████▉  | 150/189 [01:19<00:20,  1.92it/s]#015 80%|███████▉  | 151/189 [01:20<00:19,  1.92it/s]#015 80%|████████  | 152/189 [01:20<00:19,  1.93it/s]#015 81%|████████  | 153/189 [01:21<00:18,  1.93it/s]#015 81%|████████▏ | 154/189 [01:21<00:18,  1.93it/s]#015 82%|████████▏ | 155/189 [01:22<00:17,  1.93it/s]#015 83%|████████▎ | 156/189 [01:22<00:17,  1.93it/s]#015 83%|████████▎ | 157/189 [01:23<00:16,  1.93it/s]#015 84%|████████▎ | 158/189 [01:23<00:16,  1.93it/s]#015 84%|████████▍ | 159/189 [01:24<00:15,  1.92it/s]#015 85%|████████▍ | 160/189 [01:24<00:15,  1.92it/s]#015 85%|████████▌ | 161/189 [01:25<00:14,  1.92it/s]#015 86%|████████▌ | 162/189 [01:25<00:14,  1.93it/s]#015 86%|████████▌ | 163/189 [01:26<00:13,  1.93it/s]#015 87%|████████▋ | 164/189 [01:26<00:12,  1.93it/s]#015 87%|████████▋ | 165/189 [01:27<00:12,  1.93it/s]#015 88%|████████▊ | 166/189 [01:28<00:11,  1.93it/s]#015 88%|████████▊ | 167/189 [01:28<00:11,  1.93it/s]#015 89%|████████▉ | 168/189 [01:29<00:10,  1.93it/s]#015 89%|████████▉ | 169/189 [01:29<00:10,  1.93it/s]#015 90%|████████▉ | 170/189 [01:30<00:09,  1.94it/s]#015 90%|█████████ | 171/189 [01:30<00:09,  1.94it/s]#015 91%|█████████ | 172/189 [01:31<00:08,  1.94it/s]#015 92%|█████████▏| 173/189 [01:31<00:08,  1.93it/s]#015 92%|█████████▏| 174/189 [01:32<00:07,  1.93it/s]#015 93%|█████████▎| 175/189 [01:32<00:07,  1.93it/s]#015 93%|█████████▎| 176/189 [01:33<00:06,  1.93it/s]#015 94%|█████████▎| 177/189 [01:33<00:06,  1.93it/s]#015 94%|█████████▍| 178/189 [01:34<00:05,  1.93it/s]#015 95%|█████████▍| 179/189 [01:34<00:05,  1.93it/s]#015 95%|█████████▌| 180/189 [01:35<00:04,  1.92it/s]#015 96%|█████████▌| 181/189 [01:35<00:04,  1.93it/s]#015 96%|█████████▋| 182/189 [01:36<00:03,  1.93it/s]#015 97%|█████████▋| 183/189 [01:36<00:03,  1.93it/s]#015 97%|█████████▋| 184/189 [01:37<00:02,  1.94it/s]#015 98%|█████████▊| 185/189 [01:37<00:02,  1.94it/s]#015 98%|█████████▊| 186/189 [01:38<00:01,  1.94it/s]#015 99%|█████████▉| 187/189 [01:38<00:01,  1.93it/s]#015 99%|█████████▉| 188/189 [01:39<00:00,  1.94it/s]#015100%|██████████| 189/189 [01:39<00:00,  2.25it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 2/3 [00:00<00:00,  7.02it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.38it/s]#033[A#015                                                 #015\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015100%|██████████| 189/189 [01:40<00:00,  2.25it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.38it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015                                                 #015#015100%|██████████| 189/189 [01:40<00:00,  2.25it/s]#015100%|██████████| 189/189 [01:40<00:00,  1.88it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]#015 67%|██████▋   | 2/3 [00:00<00:00,  7.02it/s]#015100%|██████████| 3/3 [00:00<00:00,  5.38it/s]#015100%|██████████| 3/3 [00:00<00:00,  4.50it/s]\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-12-28 17:01:57 Uploading - Uploading generated training model\n",
      "2020-12-28 17:02:35 Completed - Training job completed\n",
      "Training seconds: 371\n",
      "Billable seconds: 111\n",
      "Managed Spot Training savings: 70.1%\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(\n",
    "    {'train': training_input_path, 'test': test_input_path},\n",
    "    experiment_config = {\n",
    "        # \"ExperimentName\"\n",
    "        \"TrialName\" : trial.trial_name,\n",
    "        \"TrialComponentDisplayName\" : \"TrainingJob\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune model with differen Hyperparamter and track them\n",
    "\n",
    "in this example we are going to train 2 different estimator with different transformers models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path  = 's3://sagemaker-eu-central-1-558105141721/datasets/imdb/small/train'\n",
    "test_input_path      = 's3://sagemaker-eu-central-1-558105141721/datasets/imdb/small/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    }
   ],
   "source": [
    "model_experiment = sess_experiment.create(experiment_name='different-model-experiment') # ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,119}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=[5e-5,1e-4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: hf-training-job-learning-rate-0-1609235016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 09:43:37 Starting - Starting the training job...\n",
      "2020-12-29 09:44:01 Starting - Launching requested ML instancesProfilerReport-1609235017: InProgress\n",
      "......\n",
      "2020-12-29 09:45:02 Starting - Preparing the instances for training.........\n",
      "2020-12-29 09:46:23 Downloading - Downloading input data\n",
      "2020-12-29 09:46:23 Training - Downloading the training image.....................\n",
      "2020-12-29 09:50:06 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-12-29 09:50:02,393 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-12-29 09:50:02,417 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-12-29 09:50:05,453 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-12-29 09:50:05,862 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 8,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 3,\n",
      "        \"learning_rate\": 5e-05,\n",
      "        \"eval_batch_size\": 16\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"hf-training-job-learning-rate-0-1609235016\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-558105141721/hf-training-job-learning-rate-0-1609235016/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":3,\"eval_batch_size\":16,\"learning_rate\":5e-05,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":8}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-558105141721/hf-training-job-learning-rate-0-1609235016/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":3,\"eval_batch_size\":16,\"learning_rate\":5e-05,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":8},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"hf-training-job-learning-rate-0-1609235016\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-558105141721/hf-training-job-learning-rate-0-1609235016/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"3\",\"--eval_batch_size\",\"16\",\"--learning_rate\",\"5e-05\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"8\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=3\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=5e-05\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_BATCH_SIZE=16\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --epochs 3 --eval_batch_size 16 --learning_rate 5e-05 --model_name distilbert-base-uncased --train_batch_size 8\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-12-29 09:50:09,547 - __main__ - INFO -  loaded train_dataset length is: 2000\u001b[0m\n",
      "\u001b[34m2020-12-29 09:50:09,547 - __main__ - INFO -  loaded test_dataset length is: 150\u001b[0m\n",
      "\u001b[34m2020-12-29 09:50:09,835 - filelock - INFO - Lock 140486089678688 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2020-12-29 09:50:10,122 - filelock - INFO - Lock 140486089678688 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2020-12-29 09:50:10,406 - filelock - INFO - Lock 140486089678072 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m2020-12-29 09:50:14,877 - filelock - INFO - Lock 140486089678072 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[2020-12-29 09:50:20.286 algo-1:25 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-12-29 09:50:20.286 algo-1:25 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-12-29 09:50:20.286 algo-1:25 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2020-12-29 09:50:20.287 algo-1:25 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2020-12-29 09:50:20.868 algo-1:25 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2020-12-29 09:50:20.868 algo-1:25 INFO hook.py:461] Hook is writing from the hook with pid: 25\n",
      "\u001b[0m\n",
      "\u001b[34m[2020-12-29 09:50:22.255 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2020-12-29 09:50:22.255 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2020-12-29 09:50:22.345 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m{'eval_loss': 0.6338126063346863, 'eval_accuracy': 0.7933333333333333, 'eval_f1': 0.7207207207207208, 'eval_precision': 0.8888888888888888, 'eval_recall': 0.6060606060606061, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.2875109314918518, 'eval_accuracy': 0.9, 'eval_f1': 0.8920863309352518, 'eval_precision': 0.8493150684931506, 'eval_recall': 0.9393939393939394, 'epoch': 2.0}\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.28652578592300415, 'eval_accuracy': 0.8733333333333333, 'eval_f1': 0.8503937007874016, 'eval_precision': 0.8852459016393442, 'eval_recall': 0.8181818181818182, 'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m{'epoch': 3.0}\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 442/442 [00:00<00:00, 460kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.13M/268M [00:00<00:06, 41.3MB/s]#015Downloading:   4%|▎         | 9.72M/268M [00:00<00:05, 44.8MB/s]#015Downloading:   6%|▌         | 15.5M/268M [00:00<00:05, 48.1MB/s]#015Downloading:   8%|▊         | 21.5M/268M [00:00<00:04, 51.0MB/s]#015Downloading:  10%|█         | 27.5M/268M [00:00<00:04, 53.5MB/s]#015Downloading:  13%|█▎        | 33.6M/268M [00:00<00:04, 55.5MB/s]#015Downloading:  15%|█▍        | 39.7M/268M [00:00<00:03, 57.2MB/s]#015Downloading:  17%|█▋        | 45.9M/268M [00:00<00:03, 58.4MB/s]#015Downloading:  19%|█▉        | 52.1M/268M [00:00<00:03, 59.4MB/s]#015Downloading:  22%|██▏       | 57.9M/268M [00:01<00:03, 59.2MB/s]#015Downloading:  24%|██▍       | 64.1M/268M [00:01<00:03, 59.9MB/s]#015Downloading:  26%|██▌       | 70.2M/268M [00:01<00:03, 60.4MB/s]#015Downloading:  29%|██▊       | 76.4M/268M [00:01<00:03, 60.7MB/s]#015Downloading:  31%|███       | 82.6M/268M [00:01<00:03, 61.1MB/s]#015Downloading:  33%|███▎      | 88.8M/268M [00:01<00:02, 61.4MB/s]#015Downloading:  35%|███▌      | 95.0M/268M [00:01<00:02, 61.6MB/s]#015Downloading:  38%|███▊      | 101M/268M [00:01<00:02, 61.8MB/s] #015Downloading:  40%|████      | 108M/268M [00:01<00:02, 62.0MB/s]#015Downloading:  42%|████▏     | 114M/268M [00:01<00:02, 62.2MB/s]#015Downloading:  45%|████▍     | 120M/268M [00:02<00:02, 61.7MB/s]#015Downloading:  47%|████▋     | 126M/268M [00:02<00:02, 58.3MB/s]#015Downloading:  49%|████▉     | 132M/268M [00:02<00:02, 59.1MB/s]#015Downloading:  52%|█████▏    | 138M/268M [00:02<00:02, 59.8MB/s]#015Downloading:  54%|█████▍    | 145M/268M [00:02<00:02, 60.5MB/s]#015Downloading:  56%|█████▋    | 151M/268M [00:02<00:01, 61.0MB/s]#015Downloading:  59%|█████▊    | 157M/268M [00:02<00:01, 61.4MB/s]#015Downloading:  61%|██████    | 163M/268M [00:02<00:01, 61.6MB/s]#015Downloading:  63%|██████▎   | 169M/268M [00:02<00:01, 61.7MB/s]#015Downloading:  66%|██████▌   | 176M/268M [00:02<00:01, 61.9MB/s]#015Downloading:  68%|██████▊   | 182M/268M [00:03<00:01, 61.1MB/s]#015Downloading:  70%|███████   | 188M/268M [00:03<00:01, 61.5MB/s]#015Downloading:  73%|███████▎  | 194M/268M [00:03<00:01, 61.6MB/s]#015Downloading:  75%|███████▍  | 201M/268M [00:03<00:01, 61.8MB/s]#015Downloading:  77%|███████▋  | 207M/268M [00:03<00:00, 61.9MB/s]#015Downloading:  79%|███████▉  | 213M/268M [00:03<00:00, 62.0MB/s]#015Downloading:  82%|████████▏ | 219M/268M [00:03<00:00, 62.1MB/s]#015Downloading:  84%|████████▍ | 225M/268M [00:03<00:00, 62.1MB/s]#015Downloading:  86%|████████▋ | 232M/268M [00:03<00:00, 62.2MB/s]#015Downloading:  89%|████████▉ | 238M/268M [00:03<00:00, 62.2MB/s]#015Downloading:  91%|█████████ | 244M/268M [00:04<00:00, 61.5MB/s]#015Downloading:  93%|█████████▎| 250M/268M [00:04<00:00, 61.6MB/s]#015Downloading:  96%|█████████▌| 257M/268M [00:04<00:00, 61.7MB/s]#015Downloading:  98%|█████████▊| 263M/268M [00:04<00:00, 61.8MB/s]#015Downloading: 100%|██████████| 268M/268M [00:04<00:00, 60.7MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/189 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /codebuild/output/src926819546/src/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\u001b[0m\n",
      "\u001b[34m#015  1%|          | 1/189 [00:02<08:36,  2.75s/it]#015  1%|          | 2/189 [00:03<06:27,  2.07s/it]#015  2%|▏         | 3/189 [00:03<04:58,  1.60s/it]#015  2%|▏         | 4/189 [00:04<03:55,  1.27s/it]#015  3%|▎         | 5/189 [00:04<03:12,  1.04s/it]#015  3%|▎         | 6/189 [00:05<02:41,  1.13it/s]#015  4%|▎         | 7/189 [00:05<02:20,  1.30it/s]#015  4%|▍         | 8/189 [00:06<02:05,  1.45it/s]#015  5%|▍         | 9/189 [00:06<01:54,  1.57it/s]#015  5%|▌         | 10/189 [00:07<01:46,  1.68it/s]#015  6%|▌         | 11/189 [00:07<01:41,  1.76it/s]#015  6%|▋         | 12/189 [00:08<01:37,  1.82it/s]#015  7%|▋         | 13/189 [00:08<01:34,  1.86it/s]#015  7%|▋         | 14/189 [00:09<01:32,  1.90it/s]#015  8%|▊         | 15/189 [00:09<01:30,  1.92it/s]#015  8%|▊         | 16/189 [00:10<01:29,  1.94it/s]#015  9%|▉         | 17/189 [00:10<01:27,  1.96it/s]#015 10%|▉         | 18/189 [00:11<01:27,  1.97it/s]#015 10%|█         | 19/189 [00:11<01:26,  1.97it/s]#015 11%|█         | 20/189 [00:12<01:25,  1.97it/s]#015 11%|█         | 21/189 [00:12<01:25,  1.97it/s]#015 12%|█▏        | 22/189 [00:13<01:24,  1.97it/s]#015 12%|█▏        | 23/189 [00:13<01:24,  1.98it/s]#015 13%|█▎        | 24/189 [00:14<01:23,  1.98it/s]#015 13%|█▎        | 25/189 [00:14<01:22,  1.98it/s]#015 14%|█▍        | 26/189 [00:15<01:22,  1.97it/s]#015 14%|█▍        | 27/189 [00:15<01:21,  1.98it/s]#015 15%|█▍        | 28/189 [00:16<01:21,  1.98it/s]#015 15%|█▌        | 29/189 [00:16<01:20,  1.98it/s]#015 16%|█▌        | 30/189 [00:17<01:20,  1.98it/s]#015 16%|█▋        | 31/189 [00:17<01:19,  1.98it/s]#015 17%|█▋        | 32/189 [00:18<01:19,  1.98it/s]#015 17%|█▋        | 33/189 [00:18<01:18,  1.98it/s]#015 18%|█▊        | 34/189 [00:19<01:18,  1.98it/s]#015 19%|█▊        | 35/189 [00:19<01:17,  1.98it/s]#015 19%|█▉        | 36/189 [00:20<01:17,  1.98it/s]#015 20%|█▉        | 37/189 [00:20<01:16,  1.98it/s]#015 20%|██        | 38/189 [00:21<01:16,  1.98it/s]#015 21%|██        | 39/189 [00:21<01:17,  1.93it/s]#015 21%|██        | 40/189 [00:22<01:16,  1.95it/s]#015 22%|██▏       | 41/189 [00:22<01:15,  1.95it/s]#015 22%|██▏       | 42/189 [00:23<01:14,  1.96it/s]#015 23%|██▎       | 43/189 [00:24<01:14,  1.96it/s]#015 23%|██▎       | 44/189 [00:24<01:13,  1.96it/s]#015 24%|██▍       | 45/189 [00:25<01:13,  1.96it/s]#015 24%|██▍       | 46/189 [00:25<01:12,  1.97it/s]#015 25%|██▍       | 47/189 [00:26<01:11,  1.97it/s]#015 25%|██▌       | 48/189 [00:26<01:11,  1.97it/s]#015 26%|██▌       | 49/189 [00:27<01:10,  1.98it/s]#015 26%|██▋       | 50/189 [00:27<01:10,  1.98it/s]#015 27%|██▋       | 51/189 [00:28<01:09,  1.98it/s]#015 28%|██▊       | 52/189 [00:28<01:09,  1.97it/s]#015 28%|██▊       | 53/189 [00:29<01:08,  1.97it/s]#015 29%|██▊       | 54/189 [00:29<01:08,  1.97it/s]#015 29%|██▉       | 55/189 [00:30<01:07,  1.97it/s]#015 30%|██▉       | 56/189 [00:30<01:07,  1.98it/s]#015 30%|███       | 57/189 [00:31<01:06,  1.98it/s]#015 31%|███       | 58/189 [00:31<01:06,  1.98it/s]#015 31%|███       | 59/189 [00:32<01:05,  1.98it/s]#015 32%|███▏      | 60/189 [00:32<01:05,  1.98it/s]#015 32%|███▏      | 61/189 [00:33<01:04,  1.98it/s]#015 33%|███▎      | 62/189 [00:33<01:04,  1.98it/s]#015 33%|███▎      | 63/189 [00:33<00:54,  2.30it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 2/3 [00:00<00:00,  7.22it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.53it/s]#033[A#015                                                #015\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015 33%|███▎      | 63/189 [00:34<00:54,  2.30it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.53it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015 34%|███▍      | 64/189 [00:35<01:25,  1.45it/s]#015 34%|███▍      | 65/189 [00:35<01:18,  1.58it/s]#015 35%|███▍      | 66/189 [00:36<01:13,  1.68it/s]#015 35%|███▌      | 67/189 [00:36<01:09,  1.76it/s]#015 36%|███▌      | 68/189 [00:37<01:06,  1.82it/s]#015 37%|███▋      | 69/189 [00:37<01:04,  1.86it/s]#015 37%|███▋      | 70/189 [00:38<01:02,  1.89it/s]#015 38%|███▊      | 71/189 [00:38<01:01,  1.91it/s]#015 38%|███▊      | 72/189 [00:39<01:00,  1.93it/s]#015 39%|███▊      | 73/189 [00:39<00:59,  1.94it/s]#015 39%|███▉      | 74/189 [00:40<00:58,  1.95it/s]#015 40%|███▉      | 75/189 [00:40<00:58,  1.96it/s]#015 40%|████      | 76/189 [00:41<00:57,  1.96it/s]#015 41%|████      | 77/189 [00:41<00:57,  1.96it/s]#015 41%|████▏     | 78/189 [00:42<00:56,  1.96it/s]#015 42%|████▏     | 79/189 [00:42<00:55,  1.97it/s]#015 42%|████▏     | 80/189 [00:43<00:55,  1.96it/s]#015 43%|████▎     | 81/189 [00:43<00:55,  1.96it/s]#015 43%|████▎     | 82/189 [00:44<00:54,  1.96it/s]#015 44%|████▍     | 83/189 [00:44<00:54,  1.96it/s]#015 44%|████▍     | 84/189 [00:45<00:53,  1.96it/s]#015 45%|████▍     | 85/189 [00:45<00:52,  1.96it/s]#015 46%|████▌     | 86/189 [00:46<00:52,  1.96it/s]#015 46%|████▌     | 87/189 [00:46<00:51,  1.97it/s]#015 47%|████▋     | 88/189 [00:47<00:51,  1.97it/s]#015 47%|████▋     | 89/189 [00:47<00:50,  1.97it/s]#015 48%|████▊     | 90/189 [00:48<00:50,  1.97it/s]#015 48%|████▊     | 91/189 [00:48<00:49,  1.97it/s]#015 49%|████▊     | 92/189 [00:49<00:49,  1.97it/s]#015 49%|████▉     | 93/189 [00:49<00:48,  1.97it/s]#015 50%|████▉     | 94/189 [00:50<00:48,  1.97it/s]#015 50%|█████     | 95/189 [00:50<00:47,  1.97it/s]#015 51%|█████     | 96/189 [00:51<00:47,  1.97it/s]#015 51%|█████▏    | 97/189 [00:51<00:46,  1.97it/s]#015 52%|█████▏    | 98/189 [00:52<00:46,  1.97it/s]#015 52%|█████▏    | 99/189 [00:52<00:45,  1.97it/s]#015 53%|█████▎    | 100/189 [00:53<00:45,  1.97it/s]#015 53%|█████▎    | 101/189 [00:53<00:44,  1.97it/s]#015 54%|█████▍    | 102/189 [00:54<00:44,  1.97it/s]#015 54%|█████▍    | 103/189 [00:54<00:43,  1.97it/s]#015 55%|█████▌    | 104/189 [00:55<00:43,  1.97it/s]#015 56%|█████▌    | 105/189 [00:55<00:42,  1.97it/s]#015 56%|█████▌    | 106/189 [00:56<00:42,  1.97it/s]#015 57%|█████▋    | 107/189 [00:57<00:41,  1.97it/s]#015 57%|█████▋    | 108/189 [00:57<00:41,  1.97it/s]#015 58%|█████▊    | 109/189 [00:58<00:40,  1.97it/s]#015 58%|█████▊    | 110/189 [00:58<00:40,  1.97it/s]#015 59%|█████▊    | 111/189 [00:59<00:39,  1.97it/s]#015 59%|█████▉    | 112/189 [00:59<00:39,  1.97it/s]#015 60%|█████▉    | 113/189 [01:00<00:38,  1.97it/s]#015 60%|██████    | 114/189 [01:00<00:38,  1.97it/s]#015 61%|██████    | 115/189 [01:01<00:37,  1.97it/s]#015 61%|██████▏   | 116/189 [01:01<00:37,  1.97it/s]#015 62%|██████▏   | 117/189 [01:02<00:36,  1.97it/s]#015 62%|██████▏   | 118/189 [01:02<00:36,  1.97it/s]#015 63%|██████▎   | 119/189 [01:03<00:35,  1.97it/s]#015 63%|██████▎   | 120/189 [01:03<00:35,  1.97it/s]#015 64%|██████▍   | 121/189 [01:04<00:34,  1.97it/s]#015 65%|██████▍   | 122/189 [01:04<00:33,  1.97it/s]#015 65%|██████▌   | 123/189 [01:05<00:33,  1.97it/s]#015 66%|██████▌   | 124/189 [01:05<00:33,  1.97it/s]#015 66%|██████▌   | 125/189 [01:06<00:32,  1.97it/s]#015 67%|██████▋   | 126/189 [01:06<00:27,  2.29it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 2/3 [00:00<00:00,  7.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.48it/s]#033[A#015                                                 #015\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015 67%|██████▋   | 126/189 [01:07<00:27,  2.29it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015 67%|██████▋   | 127/189 [01:07<00:42,  1.45it/s]#015 68%|██████▊   | 128/189 [01:08<00:38,  1.58it/s]#015 68%|██████▊   | 129/189 [01:08<00:35,  1.68it/s]#015 69%|██████▉   | 130/189 [01:09<00:33,  1.75it/s]#015 69%|██████▉   | 131/189 [01:09<00:31,  1.81it/s]#015 70%|██████▉   | 132/189 [01:10<00:30,  1.86it/s]#015 70%|███████   | 133/189 [01:10<00:29,  1.89it/s]#015 71%|███████   | 134/189 [01:11<00:28,  1.91it/s]#015 71%|███████▏  | 135/189 [01:11<00:27,  1.93it/s]#015 72%|███████▏  | 136/189 [01:12<00:27,  1.94it/s]#015 72%|███████▏  | 137/189 [01:12<00:26,  1.95it/s]#015 73%|███████▎  | 138/189 [01:13<00:26,  1.96it/s]#015 74%|███████▎  | 139/189 [01:13<00:25,  1.96it/s]#015 74%|███████▍  | 140/189 [01:14<00:24,  1.96it/s]#015 75%|███████▍  | 141/189 [01:14<00:24,  1.97it/s]#015 75%|███████▌  | 142/189 [01:15<00:23,  1.97it/s]#015 76%|███████▌  | 143/189 [01:15<00:23,  1.97it/s]#015 76%|███████▌  | 144/189 [01:16<00:22,  1.96it/s]#015 77%|███████▋  | 145/189 [01:16<00:22,  1.97it/s]#015 77%|███████▋  | 146/189 [01:17<00:21,  1.97it/s]#015 78%|███████▊  | 147/189 [01:17<00:21,  1.97it/s]#015 78%|███████▊  | 148/189 [01:18<00:20,  1.97it/s]#015 79%|███████▉  | 149/189 [01:18<00:20,  1.97it/s]#015 79%|███████▉  | 150/189 [01:19<00:19,  1.97it/s]#015 80%|███████▉  | 151/189 [01:19<00:19,  1.97it/s]#015 80%|████████  | 152/189 [01:20<00:18,  1.97it/s]#015 81%|████████  | 153/189 [01:20<00:18,  1.97it/s]#015 81%|████████▏ | 154/189 [01:21<00:17,  1.97it/s]#015 82%|████████▏ | 155/189 [01:21<00:17,  1.97it/s]#015 83%|████████▎ | 156/189 [01:22<00:16,  1.96it/s]#015 83%|████████▎ | 157/189 [01:22<00:16,  1.96it/s]#015 84%|████████▎ | 158/189 [01:23<00:15,  1.96it/s]#015 84%|████████▍ | 159/189 [01:23<00:15,  1.96it/s]#015 85%|████████▍ | 160/189 [01:24<00:14,  1.96it/s]#015 85%|████████▌ | 161/189 [01:24<00:14,  1.96it/s]#015 86%|████████▌ | 162/189 [01:25<00:13,  1.96it/s]#015 86%|████████▌ | 163/189 [01:26<00:13,  1.97it/s]#015 87%|████████▋ | 164/189 [01:26<00:12,  1.97it/s]#015 87%|████████▋ | 165/189 [01:27<00:12,  1.97it/s]#015 88%|████████▊ | 166/189 [01:27<00:11,  1.97it/s]#015 88%|████████▊ | 167/189 [01:28<00:11,  1.97it/s]#015 89%|████████▉ | 168/189 [01:28<00:10,  1.96it/s]#015 89%|████████▉ | 169/189 [01:29<00:10,  1.96it/s]#015 90%|████████▉ | 170/189 [01:29<00:09,  1.97it/s]#015 90%|█████████ | 171/189 [01:30<00:09,  1.96it/s]#015 91%|█████████ | 172/189 [01:30<00:08,  1.96it/s]#015 92%|█████████▏| 173/189 [01:31<00:08,  1.96it/s]#015 92%|█████████▏| 174/189 [01:31<00:07,  1.96it/s]#015 93%|█████████▎| 175/189 [01:32<00:07,  1.97it/s]#015 93%|█████████▎| 176/189 [01:32<00:06,  1.97it/s]#015 94%|█████████▎| 177/189 [01:33<00:06,  1.97it/s]#015 94%|█████████▍| 178/189 [01:33<00:05,  1.97it/s]#015 95%|█████████▍| 179/189 [01:34<00:05,  1.97it/s]#015 95%|█████████▌| 180/189 [01:34<00:04,  1.97it/s]#015 96%|█████████▌| 181/189 [01:35<00:04,  1.97it/s]#015 96%|█████████▋| 182/189 [01:35<00:03,  1.97it/s]#015 97%|█████████▋| 183/189 [01:36<00:03,  1.97it/s]#015 97%|█████████▋| 184/189 [01:36<00:02,  1.97it/s]#015 98%|█████████▊| 185/189 [01:37<00:02,  1.97it/s]#015 98%|█████████▊| 186/189 [01:37<00:01,  1.97it/s]#015 99%|█████████▉| 187/189 [01:38<00:01,  1.97it/s]#015 99%|█████████▉| 188/189 [01:38<00:00,  1.97it/s]#015100%|██████████| 189/189 [01:38<00:00,  2.29it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 67%|██████▋   | 2/3 [00:00<00:00,  7.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.48it/s]#033[A#015                                                 #015\u001b[0m\n",
      "\u001b[34m#015                                             #015#033[A#015100%|██████████| 189/189 [01:39<00:00,  2.29it/s]\u001b[0m\n",
      "\u001b[34m2020-12-29 09:52:01,877 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 3/3 [00:00<00:00,  5.48it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                             #033[A#015                                                 #015#015100%|██████████| 189/189 [01:39<00:00,  2.29it/s]#015100%|██████████| 189/189 [01:39<00:00,  1.89it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/3 [00:00<?, ?it/s]#015 67%|██████▋   | 2/3 [00:00<00:00,  7.14it/s]#015100%|██████████| 3/3 [00:00<00:00,  5.48it/s]#015100%|██████████| 3/3 [00:00<00:00,  4.58it/s]\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-12-29 09:52:07 Uploading - Uploading generated training model\n",
      "2020-12-29 09:52:48 Completed - Training job completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training seconds: 391\n",
      "Billable seconds: 117\n",
      "Managed Spot Training savings: 70.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: hf-training-job-learning-rate-1-1609235579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-29 09:53:00 Starting - Starting the training job...\n",
      "2020-12-29 09:53:23 Starting - Launching requested ML instancesProfilerReport-1609235579: InProgress\n",
      ".........\n",
      "2020-12-29 09:54:44 Starting - Preparing the instances for training......\n",
      "2020-12-29 09:56:04 Downloading - Downloading input data\n",
      "2020-12-29 09:56:04 Training - Training in-progress...\n",
      "2020-12-29 09:56:25 Training - Downloading the training image......"
     ]
    }
   ],
   "source": [
    "from smexperiments.trial import Trial\n",
    "import time\n",
    "\n",
    "for i, rate in enumerate(learning_rate):\n",
    "    # defines trial_name in given input\n",
    "    trial_name = f\"hf-training-job-learning-rate-{i}-{int(time.time())}\"\n",
    "    \n",
    "    # creates trial for our experiment\n",
    "    hf_trial = model_experiment.create_trial(trial_name=trial_name)\n",
    "    \n",
    "    # creates HuggingFace Estimator\n",
    "    hf_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='../scripts',\n",
    "                            sagemaker_session=sess,\n",
    "                            use_spot_instances=True,\n",
    "                            max_wait=4600, # This should be equal to or greater than max_run in seconds'\n",
    "                            max_run=3600,\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version={'transformers':'4.1.1','datasets':'1.1.3'},\n",
    "                            py_version='py3',\n",
    "                            metric_definitions=[\n",
    "                                 {'Name': 'validation:eval_loss', 'Regex':\"'eval_loss': (.*?),\"},\n",
    "                                 {'Name': 'validation:eval_accuracy', 'Regex': \"'eval_accuracy': (.*?),\"},\n",
    "                                 {'Name': 'validation:eval_f1', 'Regex': \"'eval_f1': (.*?),\"},\n",
    "                                 {'Name': 'validation:eval_precision', 'Regex': \"'eval_precision': (.*?),\"},\n",
    "                                 {'Name': 'validation:eval_recall', 'Regex': \"'eval_recall': (.*?),\"}],\n",
    "                            hyperparameters = {'epochs': 3,\n",
    "                                               'train_batch_size': 8,\n",
    "                                               'eval_batch_size': 16,\n",
    "                                               'model_name': 'distilbert-base-uncased',\n",
    "                                               'learning_rate': rate},\n",
    "                            enable_sagemaker_metrics=True)\n",
    "\n",
    "    # trains the model\n",
    "    hf_estimator.fit(\n",
    "        {'train': training_input_path, 'test': test_input_path}, \n",
    "        job_name=trial_name,\n",
    "        experiment_config={\n",
    "            \"TrialName\": hf_trial.trial_name,\n",
    "            \"TrialComponentDisplayName\": \"Training\",\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOT WORKING:  Use Sagemaker Experiments without Sagemaker \n",
    "\n",
    "you can use Sagemaker Experiments without sagemaker if you use a `Tracker` to log your metrics, inputs, parameters into Sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.tracker import Tracker\n",
    "from smexperiments.trial_component import TrialComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
      "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ValidationException) when calling the CreateTrialComponent operation: Trial Component creation is currently restricted to the SageMaker runtime. Try supplying an experiment config when creating a job instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-35e2e4e71671>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# creates tracker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mman_tracker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# log hyperparameter of learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/smexperiments/tracker.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, display_name, artifact_bucket, artifact_prefix, boto3_session, sagemaker_boto_client)\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0msagemaker_boto_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker_boto_client\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         tc = trial_component.TrialComponent.create(\n\u001b[0m\u001b[1;32m    194\u001b[0m             \u001b[0mtrial_component_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TrialComponent\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mdisplay_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisplay_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/smexperiments/trial_component.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, trial_component_name, display_name, tags, sagemaker_boto_client)\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \"\"\"\n\u001b[0;32m--> 181\u001b[0;31m         return super(TrialComponent, cls)._construct(\n\u001b[0m\u001b[1;32m    182\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boto_create_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0mtrial_component_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial_component_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/smexperiments/_base_types.py\u001b[0m in \u001b[0;36m_construct\u001b[0;34m(cls, boto_method_name, sagemaker_boto_client, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0msagemaker_boto_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker_boto_client\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msagemaker_boto_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboto_method_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwith_boto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboto_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/smexperiments/_base_types.py\u001b[0m in \u001b[0;36m_invoke_api\u001b[0;34m(self, boto_method, boto_method_members)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mapi_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_boto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mapi_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_boto_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboto_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mapi_boto_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mapi_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_boto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_boto_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    356\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.anaconda3/envs/hf/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ValidationException) when calling the CreateTrialComponent operation: Trial Component creation is currently restricted to the SageMaker runtime. Try supplying an experiment config when creating a job instead."
     ]
    }
   ],
   "source": [
    "# create sagemaker experiment\n",
    "man_experiment = sess_experiment.create(experiment_name='manual-experiment3') # ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,119}\n",
    "\n",
    "# create trial for experiment\n",
    "man_trial = man_experiment.create_trial(trial_name='manuall-trial-2')\n",
    "\n",
    "# creates tracker\n",
    "man_tracker = Tracker.create()\n",
    "\n",
    "# log hyperparameter of learning rate\n",
    "man_tracker.log_parameter('learning_rate', 0.01)\n",
    "\n",
    "# logs metrics\n",
    "man_tracker.log_metric(metric_name='accuracy', value=0.9, iteration_number=1)\n",
    "man_tracker.log_metric(metric_name='f1', value=0.7, iteration_number=1)\n",
    "\n",
    "\n",
    "# add tracked values to trial\n",
    "man_trial.add_trial_component(man_tracker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
