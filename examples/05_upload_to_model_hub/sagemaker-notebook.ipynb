{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk extension example using `Trainer` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installs requirements if you haven´t already done it and sets up ipywidgets for datasets in sagemaker studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r ../requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os \n",
    "import IPython\n",
    "if 'SAGEMAKER_TRAINING_MODULE' in os.environ:\n",
    "    !conda install -c conda-forge ipywidgets -y\n",
    "    IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install git-lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!brew install git-lfs\n",
    "# ! curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\n",
    "# !sudo apt-get install git-lfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Sagemaker Session with local AWS Profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From outside these notebooks, `get_execution_role()` will return an exception because it does not know what is the role name that SageMaker requires.\n",
    "\n",
    "To solve this issue, pass the IAM role name instead of using `get_execution_role()`.\n",
    "\n",
    "Therefore you have to create an IAM-Role with correct permission for sagemaker to start training jobs and download files from s3. Beware that you need s3 permission on bucket-level `\"arn:aws:s3:::sagemaker-*\"` and on object-level     `\"arn:aws:s3:::sagemaker-*/*\"`. \n",
    "\n",
    "You can read [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) how to create a role with right permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local aws profile configured in ~/.aws/credentials\n",
    "local_profile_name='hf-sm' # optional if you only have default configured\n",
    "\n",
    "# role name for sagemaker -> needs the described permissions from above\n",
    "role_name = \"SageMakerRole\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name philipp to get Role path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::558105141721:role/SageMakerRole\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "try:\n",
    "    sess = sagemaker.Session()\n",
    "    role = sagemaker.get_execution_role()\n",
    "except Exception:\n",
    "    import boto3\n",
    "    # creates a boto3 session using the local profile we defined\n",
    "    if local_profile_name:\n",
    "        os.environ['AWS_PROFILE'] = local_profile_name # setting env var bc local-mode cannot use boto3 session\n",
    "        #bt3 = boto3.session.Session(profile_name=local_profile_name)\n",
    "        #iam = bt3.client('iam')\n",
    "        # create sagemaker session with boto3 session\n",
    "        #sess = sagemaker.Session(boto_session=bt3)\n",
    "    iam = boto3.client('iam')\n",
    "    sess = sagemaker.Session()\n",
    "    # get role arn\n",
    "    role = iam.get_role(RoleName=role_name)['Role']['Arn']\n",
    "    \n",
    "\n",
    "\n",
    "print(role)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sagemaker Session prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['datasets/imdb/small/test/dataset.arrow', 'datasets/imdb/small/test/dataset_info.json', 'datasets/imdb/small/test/state.json', 'datasets/imdb/small/test/test_dataset.pt', 'datasets/imdb/small/train/dataset.arrow', 'datasets/imdb/small/train/dataset_info.json', 'datasets/imdb/small/train/state.json', 'datasets/imdb/small/training/train_dataset.pt', 'datasets/imdb/test/dataset.arrow', 'datasets/imdb/test/dataset_info.json', 'datasets/imdb/test/state.json', 'datasets/imdb/train/dataset.arrow', 'datasets/imdb/train/dataset_info.json', 'datasets/imdb/train/state.json']\n",
      "sagemaker-eu-central-1-558105141721\n",
      "eu-central-1\n"
     ]
    }
   ],
   "source": [
    "print(sess.list_s3_files(sess.default_bucket(),'datasets/')) # list objects in s3 under datsets/\n",
    "print(sess.default_bucket()) # s3 bucketname\n",
    "print(sess.boto_region_name) # aws region of sagemaker session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "Since we are using the `.py` module directly from `huggingface/` we have to adjust our `sys.path` to be able to import our estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "Reusing dataset imdb (/Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3)\n",
      "Loading cached shuffled indices for dataset at /Users/philippschmid/.cache/huggingface/datasets/imdb/plain_text/1.0.0/90099cb476936b753383ba2ae6ab2eae419b2e87f71cd5189cb9c8e5814d12a3/cache-f7ed38da5ada7a37.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863cd3004eb148cb97049a6bf877e35f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7c88b6319844ddea77b65e4332961ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = load_dataset('imdb')\n",
    "\n",
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "#helper tokenizer function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000)) # smaller the size for test dataset to 10k \n",
    "\n",
    "# sample a to small dataset for training\n",
    "#train_dataset = train_dataset.shuffle().select(range(2000)) # smaller the size for test dataset to 10k \n",
    "#test_dataset = test_dataset.shuffle().select(range(150)) # smaller the size for test dataset to 10k \n",
    "\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n",
    "test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset.rename_column_(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.rename_column_(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload data to sagemaker S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def upload_data_to_s3(dataset=None,prefix='datasets',split_type='train'):\n",
    "    \"\"\"helper function with saves the dataset locally using dataset.save_to_disk() and upload its then to s3. \"\"\"\n",
    "    \n",
    "    temp_prefix =f\"{prefix}/{split_type}\"\n",
    "    # saves datasets in local directory\n",
    "    dataset.save_to_disk(f\"./{temp_prefix}\")\n",
    "    \n",
    "    # loops over saved files and uploads them to s3 \n",
    "    for file in glob.glob(f\"./{temp_prefix}/*\"):\n",
    "        sess.upload_data(file, key_prefix=temp_prefix)\n",
    "\n",
    "    # return s3 url to files for estimator.fit()\n",
    "    return f\"s3://{sess.default_bucket()}/{temp_prefix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-eu-central-1-558105141721/datasets/imdb/train\n",
      "s3://sagemaker-eu-central-1-558105141721/datasets/imdb/test\n"
     ]
    }
   ],
   "source": [
    "prefix = 'datasets/imdb'\n",
    "\n",
    "training_input_path  = upload_data_to_s3(dataset=train_dataset,prefix=prefix,split_type='train')\n",
    "test_input_path      = upload_data_to_s3(dataset=test_dataset,prefix=prefix,split_type='test')\n",
    "\n",
    "print(training_input_path)\n",
    "print(test_input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Estimator\n",
    "\n",
    "The following code sample shows how you train a custom HuggingFace script `train.py`, passing in three hyperparameters (`epochs`,`train_batch_size`,`model_name`). We are not going to pass any data into sagemaker training job instead it will be downloaded in `train.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path  = \"s3://sagemaker-eu-central-1-558105141721/datasets/imdb/small/train\"\n",
    "test_input_path      = \"s3://sagemaker-eu-central-1-558105141721/datasets/imdb/small/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface.estimator import HuggingFace\n",
    "\n",
    "\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='../scripts',\n",
    "                            sagemaker_session=sess,\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            role=role,\n",
    "                            framework_version={'transformers':'4.1.1','datasets':'1.1.3'},\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n",
      "2021-01-06 16:38:17 Starting - Starting the training job...\n",
      "2021-01-06 16:38:41 Starting - Launching requested ML instancesProfilerReport-1609951097: InProgress\n",
      "......\n",
      "2021-01-06 16:39:45 Starting - Preparing the instances for training......\n",
      "2021-01-06 16:40:46 Downloading - Downloading input data...\n",
      "2021-01-06 16:41:03 Training - Downloading the training image....................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-01-06 16:44:29,297 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-01-06 16:44:29,320 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-01-06 16:44:32,340 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-01-06 16:44:32,764 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 32,\n",
      "        \"model_name\": \"distilbert-base-uncased\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-sdk-extension-2021-01-06-16-38-17-142\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2021-01-06-16-38-17-142/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2021-01-06-16-38-17-142/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"model_name\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-sdk-extension-2021-01-06-16-38-17-142\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2021-01-06-16-38-17-142/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"1\",\"--model_name\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=32\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=distilbert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-01-06 16:44:36,563 - __main__ - INFO -  loaded train_dataset length is: 2000\u001b[0m\n",
      "\u001b[34m2021-01-06 16:44:36,563 - __main__ - INFO -  loaded test_dataset length is: 2000\u001b[0m\n",
      "\u001b[34m2021-01-06 16:44:36,868 - filelock - INFO - Lock 140541347516032 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-01-06 16:44:37,155 - filelock - INFO - Lock 140541347516032 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.d423bdf2f58dc8b77d5f5d18028d7ae4a72dcfd8f468e81fe979ada957a8c361.lock\u001b[0m\n",
      "\u001b[34m2021-01-06 16:44:37,441 - filelock - INFO - Lock 140541347515920 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\n",
      "2021-01-06 16:44:46 Training - Training image download completed. Training in progress.\u001b[34m2021-01-06 16:44:42,376 - filelock - INFO - Lock 140541347515920 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\u001b[0m\n",
      "\u001b[34m[2021-01-06 16:44:47.798 algo-1:25 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2021-01-06 16:44:47.798 algo-1:25 INFO hook.py:193] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2021-01-06 16:44:47.799 algo-1:25 INFO hook.py:238] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2021-01-06 16:44:47.799 algo-1:25 INFO state_store.py:67] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2021-01-06 16:44:47.816 algo-1:25 INFO hook.py:398] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2021-01-06 16:44:47.817 algo-1:25 INFO hook.py:461] Hook is writing from the hook with pid: 25\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-01-06 16:44:49.158 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:distilbert.transformer BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-01-06 16:44:49.158 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:distilbert BaseModelOutput\u001b[0m\n",
      "\u001b[34m[2021-01-06 16:44:49.246 algo-1:25 WARNING hook.py:978] var is not Tensor or list or tuple of Tensors, module_name:DistilBertForSequenceClassification SequenceClassifierOutput\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m{'eval_loss': 0.6557341814041138, 'eval_accuracy': 0.796, 'eval_f1': 0.7838983050847458, 'eval_precision': 0.8495981630309989, 'eval_recall': 0.727630285152409, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m{'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m***** Eval results *****\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 442/442 [00:00<00:00, 392kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   1%|▏         | 3.76M/268M [00:00<00:07, 37.6MB/s]#015Downloading:   3%|▎         | 8.43M/268M [00:00<00:06, 39.9MB/s]#015Downloading:   5%|▍         | 13.3M/268M [00:00<00:06, 42.3MB/s]#015Downloading:   7%|▋         | 18.4M/268M [00:00<00:05, 44.6MB/s]#015Downloading:   9%|▉         | 23.9M/268M [00:00<00:05, 47.2MB/s]#015Downloading:  11%|█         | 29.4M/268M [00:00<00:04, 49.3MB/s]#015Downloading:  13%|█▎        | 34.9M/268M [00:00<00:04, 51.0MB/s]#015Downloading:  15%|█▌        | 40.5M/268M [00:00<00:04, 52.2MB/s]#015Downloading:  17%|█▋        | 46.3M/268M [00:00<00:04, 53.8MB/s]#015Downloading:  19%|█▉        | 52.1M/268M [00:01<00:03, 55.1MB/s]#015Downloading:  22%|██▏       | 58.0M/268M [00:01<00:03, 56.1MB/s]#015Downloading:  24%|██▍       | 63.8M/268M [00:01<00:03, 56.7MB/s]#015Downloading:  26%|██▌       | 69.4M/268M [00:01<00:03, 55.9MB/s]#015Downloading:  28%|██▊       | 75.0M/268M [00:01<00:03, 55.2MB/s]#015Downloading:  30%|███       | 80.7M/268M [00:01<00:03, 55.5MB/s]#015Downloading:  32%|███▏      | 86.2M/268M [00:01<00:03, 55.6MB/s]#015Downloading:  34%|███▍      | 91.9M/268M [00:01<00:03, 55.8MB/s]#015Downloading:  36%|███▋      | 97.4M/268M [00:01<00:03, 55.5MB/s]#015Downloading:  38%|███▊      | 103M/268M [00:01<00:02, 55.6MB/s] #015Downloading:  41%|████      | 109M/268M [00:02<00:02, 56.0MB/s]#015Downloading:  43%|████▎     | 114M/268M [00:02<00:02, 56.2MB/s]#015Downloading:  45%|████▍     | 120M/268M [00:02<00:02, 56.3MB/s]#015Downloading:  47%|████▋     | 126M/268M [00:02<00:02, 56.5MB/s]#015Downloading:  49%|████▉     | 131M/268M [00:02<00:02, 56.5MB/s]#015Downloading:  51%|█████     | 137M/268M [00:02<00:02, 56.2MB/s]#015Downloading:  53%|█████▎    | 143M/268M [00:02<00:02, 54.0MB/s]#015Downloading:  55%|█████▌    | 148M/268M [00:02<00:02, 54.5MB/s]#015Downloading:  57%|█████▋    | 154M/268M [00:02<00:02, 54.5MB/s]#015Downloading:  59%|█████▉    | 159M/268M [00:02<00:01, 55.0MB/s]#015Downloading:  62%|██████▏   | 165M/268M [00:03<00:01, 55.2MB/s]#015Downloading:  64%|██████▎   | 170M/268M [00:03<00:01, 55.3MB/s]#015Downloading:  66%|██████▌   | 176M/268M [00:03<00:01, 55.5MB/s]#015Downloading:  68%|██████▊   | 182M/268M [00:03<00:01, 55.7MB/s]#015Downloading:  70%|██████▉   | 187M/268M [00:03<00:01, 55.9MB/s]#015Downloading:  72%|███████▏  | 193M/268M [00:03<00:01, 55.8MB/s]#015Downloading:  74%|███████▍  | 199M/268M [00:03<00:01, 56.0MB/s]#015Downloading:  76%|███████▌  | 204M/268M [00:03<00:01, 56.2MB/s]#015Downloading:  78%|███████▊  | 210M/268M [00:03<00:01, 56.0MB/s]#015Downloading:  80%|████████  | 216M/268M [00:03<00:00, 56.3MB/s]#015Downloading:  83%|████████▎ | 221M/268M [00:04<00:00, 56.3MB/s]#015Downloading:  85%|████████▍ | 227M/268M [00:04<00:00, 56.6MB/s]#015Downloading:  87%|████████▋ | 233M/268M [00:04<00:00, 56.6MB/s]#015Downloading:  89%|████████▉ | 238M/268M [00:04<00:00, 56.7MB/s]#015Downloading:  91%|█████████ | 244M/268M [00:04<00:00, 56.7MB/s]#015Downloading:  93%|█████████▎| 250M/268M [00:04<00:00, 56.7MB/s]#015Downloading:  95%|█████████▌| 255M/268M [00:04<00:00, 56.9MB/s]#015Downloading:  97%|█████████▋| 261M/268M [00:04<00:00, 56.9MB/s]#015Downloading: 100%|█████████▉| 267M/268M [00:04<00:00, 56.3MB/s]#015Downloading: 100%|██████████| 268M/268M [00:04<00:00, 55.2MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/63 [00:00<?, ?it/s]/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /codebuild/output/src926819546/src/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.tensor(x, **format_kwargs)\u001b[0m\n",
      "\u001b[34m#015  2%|▏         | 1/63 [00:02<02:18,  2.23s/it]#015  3%|▎         | 2/63 [00:02<01:44,  1.71s/it]#015  5%|▍         | 3/63 [00:03<01:20,  1.35s/it]#015  6%|▋         | 4/63 [00:03<01:04,  1.09s/it]#015  8%|▊         | 5/63 [00:04<00:53,  1.09it/s]#015 10%|▉         | 6/63 [00:04<00:45,  1.26it/s]#015 11%|█         | 7/63 [00:05<00:39,  1.42it/s]#015 13%|█▎        | 8/63 [00:05<00:35,  1.54it/s]#015 14%|█▍        | 9/63 [00:06<00:32,  1.66it/s]#015 16%|█▌        | 10/63 [00:06<00:30,  1.74it/s]#015 17%|█▋        | 11/63 [00:07<00:28,  1.81it/s]#015 19%|█▉        | 12/63 [00:07<00:27,  1.86it/s]#015 21%|██        | 13/63 [00:08<00:26,  1.90it/s]#015 22%|██▏       | 14/63 [00:08<00:25,  1.93it/s]#015 24%|██▍       | 15/63 [00:09<00:24,  1.95it/s]#015 25%|██▌       | 16/63 [00:09<00:24,  1.96it/s]#015 27%|██▋       | 17/63 [00:10<00:23,  1.97it/s]#015 29%|██▊       | 18/63 [00:10<00:22,  1.98it/s]#015 30%|███       | 19/63 [00:11<00:22,  1.98it/s]#015 32%|███▏      | 20/63 [00:11<00:21,  1.98it/s]#015 33%|███▎      | 21/63 [00:12<00:21,  1.98it/s]#015 35%|███▍      | 22/63 [00:12<00:20,  1.98it/s]#015 37%|███▋      | 23/63 [00:13<00:20,  1.98it/s]#015 38%|███▊      | 24/63 [00:13<00:19,  1.98it/s]#015 40%|███▉      | 25/63 [00:14<00:19,  1.98it/s]#015 41%|████▏     | 26/63 [00:14<00:18,  1.98it/s]#015 43%|████▎     | 27/63 [00:15<00:18,  1.98it/s]#015 44%|████▍     | 28/63 [00:15<00:17,  1.98it/s]#015 46%|████▌     | 29/63 [00:16<00:17,  1.98it/s]#015 48%|████▊     | 30/63 [00:16<00:16,  1.98it/s]#015 49%|████▉     | 31/63 [00:17<00:16,  1.97it/s]#015 51%|█████     | 32/63 [00:17<00:15,  1.97it/s]#015 52%|█████▏    | 33/63 [00:18<00:15,  1.97it/s]#015 54%|█████▍    | 34/63 [00:18<00:14,  1.98it/s]#015 56%|█████▌    | 35/63 [00:19<00:14,  1.98it/s]#015 57%|█████▋    | 36/63 [00:19<00:13,  1.99it/s]#015 59%|█████▊    | 37/63 [00:20<00:13,  1.99it/s]#015 60%|██████    | 38/63 [00:20<00:12,  1.99it/s]#015 62%|██████▏   | 39/63 [00:21<00:12,  1.92it/s]#015 63%|██████▎   | 40/63 [00:21<00:11,  1.93it/s]#015 65%|██████▌   | 41/63 [00:22<00:11,  1.95it/s]#015 67%|██████▋   | 42/63 [00:22<00:10,  1.96it/s]#015 68%|██████▊   | 43/63 [00:23<00:10,  1.96it/s]#015 70%|██████▉   | 44/63 [00:23<00:09,  1.97it/s]#015 71%|███████▏  | 45/63 [00:24<00:09,  1.97it/s]#015 73%|███████▎  | 46/63 [00:24<00:08,  1.97it/s]#015 75%|███████▍  | 47/63 [00:25<00:08,  1.98it/s]#015 76%|███████▌  | 48/63 [00:25<00:07,  1.98it/s]#015 78%|███████▊  | 49/63 [00:26<00:07,  1.98it/s]#015 79%|███████▉  | 50/63 [00:26<00:06,  1.97it/s]#015 81%|████████  | 51/63 [00:27<00:06,  1.97it/s]#015 83%|████████▎ | 52/63 [00:28<00:05,  1.97it/s]#015 84%|████████▍ | 53/63 [00:28<00:05,  1.97it/s]#015 86%|████████▌ | 54/63 [00:29<00:04,  1.97it/s]#015 87%|████████▋ | 55/63 [00:29<00:04,  1.97it/s]#015 89%|████████▉ | 56/63 [00:30<00:03,  1.97it/s]#015 90%|█████████ | 57/63 [00:30<00:03,  1.98it/s]#015 92%|█████████▏| 58/63 [00:31<00:02,  1.98it/s]#015 94%|█████████▎| 59/63 [00:31<00:02,  1.98it/s]#015 95%|█████████▌| 60/63 [00:32<00:01,  1.98it/s]#015 97%|█████████▋| 61/63 [00:32<00:01,  1.98it/s]#015 98%|█████████▊| 62/63 [00:33<00:00,  1.98it/s]#015100%|██████████| 63/63 [00:33<00:00,  2.29it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/32 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  6%|▋         | 2/32 [00:00<00:04,  7.26it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015  9%|▉         | 3/32 [00:00<00:05,  5.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 12%|█▎        | 4/32 [00:00<00:05,  4.78it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 16%|█▌        | 5/32 [00:01<00:06,  4.34it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 19%|█▉        | 6/32 [00:01<00:06,  4.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 22%|██▏       | 7/32 [00:01<00:06,  3.92it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 25%|██▌       | 8/32 [00:01<00:06,  3.80it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 28%|██▊       | 9/32 [00:02<00:06,  3.73it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 31%|███▏      | 10/32 [00:02<00:05,  3.68it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 34%|███▍      | 11/32 [00:02<00:05,  3.65it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 38%|███▊      | 12/32 [00:03<00:05,  3.62it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 41%|████      | 13/32 [00:03<00:05,  3.61it/s]#033[A\u001b[0m\n",
      "\u001b[34m2021-01-06 16:45:39,833 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m#015 44%|████▍     | 14/32 [00:03<00:05,  3.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 47%|████▋     | 15/32 [00:03<00:04,  3.59it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 50%|█████     | 16/32 [00:04<00:04,  3.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 53%|█████▎    | 17/32 [00:04<00:04,  3.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 56%|█████▋    | 18/32 [00:04<00:03,  3.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 59%|█████▉    | 19/32 [00:05<00:03,  3.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 62%|██████▎   | 20/32 [00:05<00:03,  3.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 66%|██████▌   | 21/32 [00:05<00:03,  3.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 69%|██████▉   | 22/32 [00:05<00:02,  3.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 72%|███████▏  | 23/32 [00:06<00:02,  3.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 75%|███████▌  | 24/32 [00:06<00:02,  3.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 78%|███████▊  | 25/32 [00:06<00:01,  3.58it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 81%|████████▏ | 26/32 [00:06<00:01,  3.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 84%|████████▍ | 27/32 [00:07<00:01,  3.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 88%|████████▊ | 28/32 [00:07<00:01,  3.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 91%|█████████ | 29/32 [00:07<00:00,  3.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 94%|█████████▍| 30/32 [00:08<00:00,  3.57it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015 97%|█████████▋| 31/32 [00:08<00:00,  3.56it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 32/32 [00:08<00:00,  3.54it/s]#033[A#015                                               #015\u001b[0m\n",
      "\u001b[34m#015                                               #015#033[A#015100%|██████████| 63/63 [00:42<00:00,  2.29it/s]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 32/32 [00:08<00:00,  3.54it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015                                               #033[A#015                                               #015#015100%|██████████| 63/63 [00:42<00:00,  2.29it/s]#015100%|██████████| 63/63 [00:42<00:00,  1.49it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/32 [00:00<?, ?it/s]#015  6%|▋         | 2/32 [00:00<00:04,  7.15it/s]#015  9%|▉         | 3/32 [00:00<00:05,  5.49it/s]#015 12%|█▎        | 4/32 [00:00<00:05,  4.73it/s]#015 16%|█▌        | 5/32 [00:01<00:06,  4.31it/s]#015 19%|█▉        | 6/32 [00:01<00:06,  4.05it/s]#015 22%|██▏       | 7/32 [00:01<00:06,  3.89it/s]#015 25%|██▌       | 8/32 [00:01<00:06,  3.79it/s]#015 28%|██▊       | 9/32 [00:02<00:06,  3.71it/s]#015 31%|███▏      | 10/32 [00:02<00:06,  3.67it/s]#015 34%|███▍      | 11/32 [00:02<00:05,  3.63it/s]#015 38%|███▊      | 12/32 [00:03<00:05,  3.61it/s]#015 41%|████      | 13/32 [00:03<00:05,  3.59it/s]#015 44%|████▍     | 14/32 [00:03<00:05,  3.58it/s]#015 47%|████▋     | 15/32 [00:03<00:04,  3.58it/s]#015 50%|█████     | 16/32 [00:04<00:04,  3.57it/s]#015 53%|█████▎    | 17/32 [00:04<00:04,  3.57it/s]#015 56%|█████▋    | 18/32 [00:04<00:03,  3.56it/s]#015 59%|█████▉    | 19/32 [00:05<00:03,  3.56it/s]#015 62%|██████▎   | 20/32 [00:05<00:03,  3.56it/s]#015 66%|██████▌   | 21/32 [00:05<00:03,  3.56it/s]#015 69%|██████▉   | 22/32 [00:05<00:02,  3.55it/s]#015 72%|███████▏  | 23/32 [00:06<00:02,  3.55it/s]#015 75%|███████▌  | 24/32 [00:06<00:02,  3.55it/s]#015 78%|███████▊  | 25/32 [00:06<00:01,  3.55it/s]#015 81%|████████▏ | 26/32 [00:07<00:01,  3.55it/s]#015 84%|████████▍ | 27/32 [00:07<00:01,  3.55it/s]#015 88%|████████▊ | 28/32 [00:07<00:01,  3.55it/s]#015 91%|█████████ | 29/32 [00:07<00:00,  3.55it/s]#015 94%|█████████▍| 30/32 [00:08<00:00,  3.56it/s]#015 97%|█████████▋| 31/32 [00:08<00:00,  3.55it/s]#015100%|██████████| 32/32 [00:08<00:00,  3.55it/s]#015100%|██████████| 32/32 [00:08<00:00,  3.64it/s]\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-01-06 16:45:46 Uploading - Uploading generated training model\n",
      "2021-01-06 16:46:47 Completed - Training job completed\n",
      "ProfilerReport-1609951097: NoIssuesFound\n",
      "Training seconds: 341\n",
      "Billable seconds: 341\n",
      "uploading model to hub\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit({'train': training_input_path, 'test': test_input_path},)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload model to manually (soon integrated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logn into HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers.hf_api import HfApi\n",
    "from huggingface.HfRepository import HfRepository\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username: philschmid\n",
      "password: ········\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "username = input(\"username: \")\n",
    "password = getpass.getpass(\"password: \")\n",
    "\n",
    "huggingface_token = HfApi().login(username, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Repository for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_name=\"sagemaker-test-123\"\n",
    "\n",
    "repo_url = HfApi().create_repo(token=huggingface_token,name=repo_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize HF Repository API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "git version 2.24.2 (Apple Git-127)\n",
      "git-lfs/2.13.1 (GitHub; darwin amd64; go 1.15.5)\n"
     ]
    }
   ],
   "source": [
    "!git --version\n",
    "!git lfs --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"./model/huggingface-sdk-extension-2021-01-06-16-38-17-142\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_repo = HfRepository(repo_url=repo_url,huggingface_token=huggingface_token,model_dir=model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**create model card**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo.create_model_card(dataset=\"imdb\", \n",
    "                             model_id=huggingface_estimator.latest_training_job.name,\n",
    "                             hyperparameters=huggingface_estimator.hyperparameters(),\n",
    "                             eval_results={\"nothing\":'logged'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "---\r\n",
      "tags:\r\n",
      "- sagemaker\r\n",
      "datasets:\r\n",
      "- imdb\r\n",
      "---\r\n",
      "## huggingface-sdk-extension-2021-01-06-16-38-17-142 Trained from SageMaker HuggingFace extension.\r\n",
      "\r\n",
      "#### Hyperparameters\r\n",
      "```json\r\n",
      "{'epochs': '1', 'train_batch_size': '32', 'model_name': '\"distilbert-base-uncased\"', 'sagemaker_submit_directory': '\"s3://sagemaker-eu-central-1-558105141721/huggingface-sdk-extension-2021-01-06-16-38-17-142/source/sourcedir.tar.gz\"', 'sagemaker_program': '\"train.py\"', 'sagemaker_container_log_level': '20', 'sagemaker_job_name': '\"huggingface-sdk-extension-2021-01-06-16-38-17-142\"', 'sagemaker_region': '\"eu-central-1\"'}\r\n",
      "```\r\n",
      "\r\n",
      "#### Eval\r\n",
      "| key | value |\r\n",
      "| --- | ----- |\r\n",
      "| nothing | logged |\r\n"
     ]
    }
   ],
   "source": [
    "!cat {model_dir}/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_repo.commit_files_and_push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
